# -*- coding: utf-8 -*-
"""AMP_Sorter_raw_function.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rCae89Za733SrI1aiRUATzEMSvM_Sd1a
"""

!pip install biopython==1.83

!pip install pymsaviz

!sudo apt install clustalo

!apt-get update
!apt-get install -y ncbi-blast+

#### for the model weights
from google.colab import drive
drive.mount('/content/drive')

!makeblastdb \
  -in /content/drive/MyDrive/AMPDB_Master_Dataset.fasta \
  -dbtype prot \
  -out /content/ampdb_v1_db

# If your peptide is in a Python variable `x`, keep this; else set it here:
query_seq = "VCSCRLVFCRRTELRVGNCLIGGVSFTYCCTRV"
### Defensin-like protein 242 "HEEPKVAPTEELMFAPSNQPRYCRSRQVFDGSCTDRGTPRTTCFLDFLGARSASEMPKNCDCTPQPNNKRLCECSVICTDCCVKN"
##hewl="KVFGRCELAAAMKRHGLDNYRGYSLGNWVCAAKFESNFNTQATNRNTDGSTDYGILQINSRWWCNDGRTPGSRNLCNIPCSALLSSDITASVNCAKKIVSDGNGMNAWVAWRNRCKGTDVQAWIRGCRL"
## sankin_1 "GSSFCDSKCKLRCSKAGLADRCLKYCGICCEECKCVPSGTYGNKHECPCYRDKKNSKGKSKCP"
##ppdef1"AKVCTKPSKFFKGLCGTDGACTTACRKEGLHSGYCQLKGFLNSVCVCRKHC"
##hnp4="VCSCRLVFCRRTELRVGNCLIGGVSFTYCCTRV"

# 1) Write FASTA query
with open("hnp4.faa", "w") as f:
    f.write(">query_seq\n" + query_seq + "\n")

# 2) Run BLAST with a robust, 10-field tabular format
# Fields: qseqid sacc pident length evalue bitscore qcovs qseq sseq stitle
import subprocess, shlex, textwrap, os, sys

blast_cmd = textwrap.dedent("""
blastp -query hnp4.faa -db ampdb_v1_db \
  -task blastp-short -matrix BLOSUM62 -gapopen 11 -gapextend 1 -word_size 2 \
  -seg no -comp_based_stats 0 -evalue 500 -max_target_seqs 2000 \
  -outfmt "6 qseqid sacc pident length evalue bitscore score qcovs qstart qend sstart send qseq sseq stitle"
""").strip()

# Run and capture to file (safer than a pipe for later parsing)
with open("hits_with_seqs.tsv", "w") as out, open("blast_errors.txt", "w") as err:
    p = subprocess.run(shlex.split(blast_cmd), stdout=out, stderr=err, text=True)
    if p.returncode != 0:
        print("BLAST exited with non-zero status. See blast_errors.txt")

import pandas as pd
query_seq = "VCSCRLVFCRRTELRVGNCLIGGVSFTYCCTRV"

cols = [
    "qseqid","sacc","pident","length","evalue","bitscore","score",
    "qcovs","qstart","qend","sstart","send","qseq","sseq","stitle"
]
df = pd.read_csv("hits_with_seqs.tsv", sep="\t", header=None, names=cols, dtype=str)

# ---- numerics ----
for c in ["pident","length","evalue","bitscore","qcovs"]:
    df[c] = pd.to_numeric(df[c], errors="coerce")

# ---- helpers ----
def ungap(x: str) -> str:
    return "".join(ch for ch in str(x).strip() if ch not in "- \t\r\n")

def pid_gapless(q: str, s: str) -> float:
    qg, sg = ungap(q), ungap(s)
    if not qg or not sg:
        return 0.0
    L = min(len(qg), len(sg))
    matches = sum(1 for a, b in zip(qg[:L], sg[:L]) if a == b)
    return 100.0 * matches / L

# ---- compute derived fields ----
df["qseq_ng"] = df["qseq"].map(ungap)
df["sseq_ng"] = df["sseq"].map(ungap)
df["pid_calc"] = [pid_gapless(q, s) for q, s in zip(df["qseq"], df["sseq"])]

# No deduplication to keep all possible homologs!
df_sorted = df.sort_values(
    ["evalue", "bitscore", "pid_calc", "length", "qcovs"],
    ascending=[True, False, False, False, False]
)

# ---- FILTERS (relaxed for 20‚Äì50 hits) ----
query_len = len(query_seq)

filtered = df_sorted[
    (df_sorted["pid_calc"] >= 30) &                     # identity ‚â•12%
    (df_sorted["evalue"] <= 1e-2) &                        # allow weaker hits
    (df_sorted["length"] >= (query_len * 0.8))         # 80% coverage
].copy()

# Final sorting for quality
filtered = filtered.sort_values(
    ["evalue", "bitscore", "pid_calc"],
    ascending=[True, False, False]
)

print("Filtered hits:", len(filtered))
filtered.tail(10)

query_seq = "VCSCRLVFCRRTELRVGNCLIGGVSFTYCCTRV"

# Step 1: Add query to the top of your sequences list
from Bio.Seq import Seq
from Bio.SeqRecord import SeqRecord
from Bio import AlignIO
import tempfile
import os
import matplotlib.pyplot as plt
import pandas as pd
import subprocess
import shlex

records = [SeqRecord(Seq(query_seq), id="query")] + [
    SeqRecord(Seq(r['sseq']), id=f"{r.sacc}_{i}") for i, (_, r) in enumerate(filtered.iterrows())
]

# Save to FASTA
with tempfile.NamedTemporaryFile(mode="w+", delete=False, suffix=".fasta") as f:
    for rec in records:
        f.write(f">{rec.id}\n{rec.seq}\n")
    fasta_path = f.name

# Run Clustal Omega using subprocess
aligned_path = fasta_path.replace(".fasta", "_aligned.fasta")
# Construct the command as a list of strings for shlex.split
clustalomega_cmd = f"clustalo -i {fasta_path} -o {aligned_path} --force --threads=2"

try:
    subprocess.run(shlex.split(clustalomega_cmd), check=True)
except subprocess.CalledProcessError as e:
    print(f"Error running Clustal Omega: {e}")
    # Optional: print stderr if you want more details
    # print(e.stderr)


# Load alignment
alignment = AlignIO.read(aligned_path, "fasta")
os.remove(fasta_path)
os.remove(aligned_path)

# Step 2: Identify aligned query and get its index
query_aligned = None
for rec in alignment:
    if rec.id == "query":
        query_aligned = rec.seq
        break

assert query_aligned is not None, "Query sequence not found in alignment!"

# Step 3: Compute conservation at each position of the query (non-gap positions only)
conservation_scores = []
query_positions = []
aa_at_positions = []

for i, aa in enumerate(query_aligned):
    if aa == "-":
        continue  # Skip gaps in query
    # Compare to other sequences at this aligned position
    matches = sum(1 for rec in alignment if rec.id != "query" and rec.seq[i] == aa)
    total = sum(1 for rec in alignment if rec.id != "query" and rec.seq[i] != "-")
    score = matches / total if total > 0 else 0
    conservation_scores.append(score)
    query_positions.append(len(query_positions) + 1)
    aa_at_positions.append(aa)

# Create DataFrame
query_conservation_df = pd.DataFrame({
    "Position": query_positions,
    "Amino_Acid": aa_at_positions,
    "Conservation": conservation_scores
})

plt.figure(figsize=(12, 5))
plt.bar(query_conservation_df["Position"], query_conservation_df["Conservation"],
        tick_label=query_conservation_df["Amino_Acid"], color="skyblue")
plt.xlabel("Position in Query Sequence")
plt.ylabel("Identity with Aligned Sequences")
plt.title("Amino Acid Conservation in Query Sequence")
plt.grid(axis="y")
plt.tight_layout()
plt.show()

import pandas as pd

query_seq = "VCSCRLVFCRRTELRVGNCLIGGVSFTYCCTRV"

# --- 1) Keep rows with Conservation ‚â• 0.8 ---
mask = pd.to_numeric(query_conservation_df["Conservation"], errors="coerce") >= 0.8
tmp = query_conservation_df.loc[mask, ["Position", "Amino_Acid"]].dropna(subset=["Position","Amino_Acid"]).copy()

# --- 2) Clean types ---
tmp["Position"] = pd.to_numeric(tmp["Position"], errors="coerce")
tmp = tmp.dropna(subset=["Position"])
tmp["Position"] = tmp["Position"].astype(int) - 1      # convert to 0-based index
tmp["Amino_Acid"] = tmp["Amino_Acid"].astype(str).str.strip()

# --- 3) If same Position appears multiple times, take most common AA ---
aa_by_pos = tmp.groupby("Position")["Amino_Acid"] \
    .agg(lambda s: s.mode().iat[0] if not s.mode().empty else s.iloc[0]) \
    .sort_index()

# --- 4) Build the dictionary ---
pos_to_evolution = aa_by_pos.to_dict()

# -----------------------------------------------------------
# --- NEW STEP: Ensure ALL CYSTEINES ARE INCLUDED (even <0.8)
# -----------------------------------------------------------
for i, aa in enumerate(query_seq):
    if aa == "C":                                  # if the query position is cysteine
        if i not in pos_to_evolution:              # and this position was not conserved
            pos_to_evolution[i] = "C"              # force-add it

pos_to_evolution

len(pos_to_evolution)

len(query_seq)

# --- install once (terminal) ---
# pip install pymsaviz biopython
# (optional) conda install -c conda-forge -c bioconda pymsaviz muscle

import os, sys, shutil, subprocess
from pathlib import Path
import pandas as pd

# ----- 0) pick your table -----
data = filtered if 'filtered' in globals() else df

# safety: we need these columns
need = {"qseqid","sacc","stitle","qseq_ng","sseq_ng","pid_calc"}
missing = need - set(map(str, data.columns))
if missing:
    raise ValueError(f"Missing columns in DataFrame: {missing}")

# ----- 1) collect sequences (Query + unique subjects) -----
# Try to load your full query from the fasta used in BLAST (change path if needed).
from Bio import SeqIO
QUERY_FASTA = "ppdef1.faa"

if Path(QUERY_FASTA).exists():
    qrec = next(SeqIO.parse(QUERY_FASTA, "fasta"))
    query_seq  = str(qrec.seq)
    # ‚¨áÔ∏è Force the label used in the MSA/PNG
    query_name = "query_seq_hnp4"
else:
    # Fallback: use the longest ungapped qseq seen in the table (local HSP slice).
    row_longest = data.loc[data["qseq_ng"].str.len().idxmax()]
    query_seq  = row_longest["qseq_ng"]
    # ‚¨áÔ∏è Force the label even in fallback mode
    query_name = "query_seq_Ppdef1"
    print("NOTE: using longest qseq_ng as the query. Provide hnp4.faa to use the full sequence.", file=sys.stderr)

# Make labels compact and unique; use ungapped subject sequences for MSA.
def mk_label(r):
    base = str(r["sacc"]).replace("|", "_")
    pid  = f"{float(r['pid_calc']):.0f}"
    return f"{base}_id{pid}"

records = [(query_name, query_seq)]

seen = set([query_seq])
for _, r in data.iterrows():
    s = str(r["sseq_ng"])
    if s and s not in seen:
        seen.add(s)
        records.append((mk_label(r), s))

# Optionally limit to top N (uncomment to keep plot tidy)
records = records[:1 + 61]  # query + top 60

# Write FASTA
FA_IN  = "selected_for_msa.fasta"
with open(FA_IN, "w") as fh:
    for name, seq in records:
        fh.write(f">{name}\n{seq}\n")

print(f"FASTA written: {FA_IN}  (n={len(records)})")

# ----- 2) run a multiple alignment (MUSCLE v5/v3 or Clustal Omega) -----
FA_OUT = "selected.aln.fasta"

def run_alignment(in_fa, out_fa):
    if shutil.which("muscle"):
        # Detect MUSCLE version; v5 uses -align/-output, v3 uses -in/-out
        try:
            ver = subprocess.run(["muscle","-version"], capture_output=True, text=True)
            if "5." in (ver.stdout + ver.stderr):
                cmd = ["muscle", "-align", in_fa, "-output", out_fa]
            else:
                cmd = ["muscle", "-in", in_fa, "-out", out_fa]
        except Exception:
            cmd = ["muscle", "-in", in_fa, "-out", out_fa]
    elif shutil.which("clustalo"):
        cmd = ["clustalo", "-i", in_fa, "-o", out_fa, "--force", "--threads=2"]
    else:
        raise RuntimeError("Please install MUSCLE or Clustal Omega (add it to PATH).")
    print("Running:", " ".join(cmd))
    subprocess.run(cmd, check=True)

run_alignment(FA_IN, FA_OUT)
print(f"Aligned MSA: {FA_OUT}")

# ----- 3) visualize with pyMSAviz -----
from pymsaviz import MsaViz

# Common nice settings: Clustal colors, consensus bar, counts, wrapped lines.
mv = MsaViz(
    FA_OUT,
    color_scheme="Clustal",
    wrap_length=60,
    show_consensus=True,
    show_count=True,
    show_grid=False,

)

# Render, title, save
fig = mv.plotfig(dpi=100)                                   # <-- returns a matplotlib Figure
fig.suptitle("Multiple Sequence Alignment-HNP-4", fontsize=14, fontweight="bold", y=1.05)  # <-- add title
fig.savefig("hnp4_top_61.png", dpi=100, bbox_inches="tight")              # <-- save PNG
fig.savefig("hnp4_top_61.pdf", bbox_inches="tight")                       # <-- save PDF
print("Saved: hnp4_top_61.png /hnp4_top_61.pdf")

len_pos_to_evolution=len(pos_to_evolution)
len_pos_to_evolution

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/W1V1995/AMP_Project.git
# 2. Go into the cloned folder
# %cd AMP_Project

# 3. Remove the requirements.txt file
!rm -f requirements.txt

# Copy your adapted version  of requirements.txt file uploaded to colab into the project directory
## /content/requirements.txt

# Commented out IPython magic to ensure Python compatibility.
# %cd /content

#### Upload updated requirments from your PC to content ( delete requirments.txt from AMP_Project folder)

!pip install -r requirements.txt

# ==========================================================
# Generate unique masks √ó variants
# Save to CSV + FASTA
# Fully compatible with AMPSorter_predictor_corrected.py
# ==========================================================

import random
import math
import pandas as pd
from math import comb

# ----------------------------------------------------------
# Function: generate N unique masks with k extra positions
# ----------------------------------------------------------
def generate_unique_masks(pos_to_evolution, seq_len, num_masks, k):
    fixed_positions = list(pos_to_evolution.keys())
    remaining_positions = [i for i in range(seq_len) if i not in fixed_positions]

    # Check if possible
    max_unique = comb(len(remaining_positions), k)
    if num_masks > max_unique:
        raise ValueError(f"Cannot generate {num_masks} unique masks. "
                         f"Maximum possible = {max_unique}.")

    masks = set()
    attempts = 0
    max_attempts = num_masks * 30  # safety loop

    while len(masks) < num_masks:
        extra = tuple(sorted(random.sample(remaining_positions, k)))
        full_mask = tuple(sorted(fixed_positions + list(extra)))
        masks.add(full_mask)

        attempts += 1
        if attempts > max_attempts:
            raise RuntimeError(
                "Could not generate enough unique masks. "
                "Increase remaining positions or decrease k."
            )

    # Convert tuple masks ‚Üí list masks
    return [list(m) for m in masks]


# ----------------------------------------------------------
# Input data
# ----------------------------------------------------------
query_seq = "VCSCRLVFCRRTELRVGNCLIGGVSFTYCCTRV"
seq_len = len(query_seq)

# Example pos_to_evolution
# Must be a dict like: {0:1, 2:1, 4:1, ...}
print("Fixed evolutionary positions:", pos_to_evolution)

# Amino acid sampling distribution
DIST = {
    'K': 1/11, 'R': 1/11, 'H': 1/11, 'L': 1/11, 'I': 1/11,
    'V': 1/11, 'F': 1/11, 'W': 1/11, 'A': 1/11, 'G': 1/11, 'P': 1/11
}
aa_choices = list(DIST.keys())
aa_probs = list(DIST.values())

# ----------------------------------------------------------
# Mask settings
# ----------------------------------------------------------
MASK_TARGET_SIZE = math.ceil(seq_len / 2)
extra_needed = MASK_TARGET_SIZE - len(pos_to_evolution)
m=seq_len-len(pos_to_evolution)

# Calculate 22 choose 6
result = math.comb(m, extra_needed)
print(result)  # Output: 170544
total_masks =  74613 ### = results            # number of unique masks to generate
variants_per_mask = 50      # variants per mask = 50
total_variants = total_masks * variants_per_mask

print("Sequence length:", seq_len)
print("Mask target size:", MASK_TARGET_SIZE)
print("Fixed positions:", len(pos_to_evolution))
print("Extra needed per mask:", extra_needed)
print("Generating", total_masks, "unique masks")

# ----------------------------------------------------------
# Generate masks (UNIQUE guaranteed)
# ----------------------------------------------------------
all_masks = generate_unique_masks(
    pos_to_evolution=pos_to_evolution,
    seq_len=seq_len,
    num_masks=total_masks,
    k=extra_needed
)

print("Generated", len(all_masks), "unique masks.")


# ----------------------------------------------------------
# Variant generation
# ----------------------------------------------------------
records = []
fasta_lines = []

for mask_i, full_mask in enumerate(all_masks):

    for var_j in range(variants_per_mask):
        seq_list = list(query_seq)

        # Fill sequence
        for pos in range(seq_len):
            if pos in full_mask:
                seq_list[pos] = query_seq[pos]  # keep original
            else:
                seq_list[pos] = random.choices(aa_choices, weights=aa_probs)[0]

        seq_generated = "".join(seq_list)
        ID = f"mask{mask_i}_var{var_j}"

        # Add row to CSV
        records.append({
            "ID": ID,
            "Sequence": seq_generated,
            "mask": full_mask,
            "fixed_positions": list(pos_to_evolution.keys())
        })

        # Add to FASTA
        fasta_lines.append(f">{ID}")
        fasta_lines.append(seq_generated)


df = pd.DataFrame(records)

# ----------------------------------------------------------
# Save to disk
# ----------------------------------------------------------
csv_path = "/content/masks_variants.csv"
fasta_path = "/content/masks_variants.fasta"

df.to_csv(csv_path, index=False)

with open(fasta_path, "w") as f:
    f.write("\n".join(fasta_lines))

print("CSV saved:", csv_path)
print("FASTA saved:", fasta_path)
print("DONE.")

#### upload : AMPSorter_predictor_corrected_fast

##### run masks and plot them ...

!python /content/AMPSorter_predictor_corrected_fast.py \
  --raw_data_path "/content/masks_variants.csv" \
  --model_path "/content/drive/MyDrive/ProteoGPT/" \
  --classifier_path "/content/drive/MyDrive/model.pt" \
  --output_path "/content/masks_variants_hnp4.csv" \
  --candidate_amp_path "/content/masks_variants_candidate_amps_hnp4.csv" \
  --batch_size 256 \
  --num_workers 4 \
  --max_length 50

df_masks_before = pd.read_csv("masks_variants.csv")
df_masks_after = pd.read_csv("masks_variants_hnp4.csv")

#### plot and select optimal mask

# ‚úÖ Merge two dataframes on "ID" using INNER JOIN
merged_df = pd.merge(df_masks_before, df_masks_after, on="ID", how="inner")

merged_df

# Save dataframe as CSV
output_path = "merged_df_raw_information.csv"
merged_df.to_csv(output_path, index=False)

# Download the file
files.download(output_path)

import numpy as np
import pandas as pd

df = merged_df.copy()

# Ensure probabilities are numeric
df["Predicted Probabilities"] = pd.to_numeric(df["Predicted Probabilities"], errors="coerce")

# Extract mask ID
df["mask_id"] = df["ID"].str.extract(r"(mask\d+)")

# Compute mean per mask
mask_stats = []
for mask_id, group in df.groupby("mask_id"):
    probs = group["Predicted Probabilities"].values
    avg_prob = probs.mean()
    mask_stats.append([mask_id, avg_prob])

mask_stats_df = pd.DataFrame(mask_stats,
    columns=["mask_id", "Variants Average Predicted Probability"])

# Extract metadata
unique_cols = df[["mask_id", "mask", "fixed_positions", "Predicted Labels"]].drop_duplicates("mask_id")

# Merge
final_df = pd.merge(unique_cols, mask_stats_df, on="mask_id", how="inner")

# Sort by descending mean
final_df = final_df.sort_values(by="Variants Average Predicted Probability", ascending=False)

# Select top 100 masks
top_mask_row = final_df.head(1)

top_mask_row

import ast

# 1Ô∏è‚É£ Get first row from the dataframe
optimal_mask = top_mask_row.iloc[0]

# 2Ô∏è‚É£ Convert to dictionary (optional, but useful)
mask_dict = optimal_mask.to_dict()

# 3Ô∏è‚É£ Parse the "mask" string into Python list
mask_list = ast.literal_eval(mask_dict["mask"])

# 4Ô∏è‚É£ Convert list to a set
mask_set = set(mask_list)

# 5Ô∏è‚É£ (Optional) sort the set for readability
mask_sorted = sorted(mask_set)

# 6Ô∏è‚É£ Print results
print("\nRaw set:")
print(mask_set)

print("\nSorted list:")
print(mask_sorted)

##### save and downlowad as df_masks_ranked
from google.colab import files

# Save dataframe as CSV
output_path = "optimal_mask_full_information.csv"
optimal_mask.to_csv(output_path, index=False)

# Download the file
files.download(output_path)

import matplotlib.pyplot as plt
import numpy as np

# Extract means
means = final_df["Variants Average Predicted Probability"].values
x = np.arange(len(final_df))

# Identify top mask
best_idx = np.argmax(means)
best_x = best_idx
best_y = means[best_idx]
best_mask_id = final_df.iloc[best_idx]["mask_id"]

plt.figure(figsize=(14, 6))

# Plot all mean probabilities
plt.plot(x, means, 'o', markersize=3)

# Annotate top mask with a long brown arrow
plt.annotate(
    f"{best_mask_id}",
    xy=(best_x, best_y),
    xytext=(best_x + 300, best_y + 0.05),  # much farther right, higher up
    arrowprops=dict(
        arrowstyle="->",
        lw=2.2,
        color="saddlebrown"      # brown arrow
    ),
    fontsize=10,
    color="saddlebrown",
    fontweight='bold'
)

plt.title("Per-Mask probability (sorted by predicted average probability)")
plt.xlabel("Rank by average probability (1 = best)")
plt.ylabel("Mean predicted probability")

plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

top_mask_row

import random
import ast

AA_POOL = ['K', 'R', 'H', 'L', 'I', 'V', 'F', 'W', 'A', 'G', 'P']

# Use only the top mask row
row = top_mask_row

mask_id = row["mask_id"].iloc[0]  # e.g., "mask65652"

# Extract the string
mask_str = row["mask"].iloc[0]

# Convert "[1, 3, ...]" ‚Üí [1, 3, ...] ‚Üí set
mask_positions = set(ast.literal_eval(mask_str))

seq = "VCSCRLVFCRRTELRVGNCLIGGVSFTYCCTRV"  # WT sequence

seq_len = len(seq)
unmasked = [i for i in range(seq_len) if i not in mask_positions]

print(f"Generating variants for {mask_id} ...")

all_variants = []

for i in range(1_000_000):  # 1 million
    var_list = list(seq)

    # mutate only unmasked positions
    for pos in unmasked:
        var_list[pos] = random.choice(AA_POOL)

    new_seq = "".join(var_list)

    variant_id = f"{mask_id}_variant_{i+1}"

    all_variants.append((variant_id, new_seq))

print("DONE")

import csv

with open("variants_top_mask.csv", "w", newline="") as f:
    writer = csv.writer(f)
    writer.writerow(["ID", "Sequence"])  # header

    # write rows in streamed chunks
    for variant_id, seq in all_variants:
        writer.writerow([variant_id, seq])

import ast

# =============================
# 1) Extract top mask row
# =============================
row = top_mask_row  # dataframe with 1 row

# Get mask string from the first row
mask_str = row["mask"].iloc[0]

# Convert string "[1, 3, ...]" -> Python list -> set
mask_positions = set(ast.literal_eval(mask_str))

# =============================
# 2) Sort for readability
# =============================
sorted_positions = sorted(mask_positions)

# =============================
# 3) Print to screen
# =============================
print("\nMask positions (sorted):")
print(sorted_positions)

# Convert to formatted string { 1, 3, 4, ... }
formatted_string = "{ " + ", ".join(str(x) for x in sorted_positions) + " }"

print("\nFormatted:")
print(formatted_string)

# =============================
# 4) Save to TXT
# =============================
with open("top_mask_positions.txt", "w") as f:
    f.write(formatted_string)

print("\nSaved to top_mask_positions.txt")

# =============================
# 5) OPTIONAL: save one per line
# =============================
with open("top_mask_positions_list.txt", "w") as f:
    for pos in sorted_positions:
        f.write(str(pos) + "\n")

print("Saved to top_mask_positions_list.txt")

!python /content/AMPSorter_predictor_corrected_fast.py \
  --raw_data_path "/content/variants_top_mask.csv" \
  --model_path "/content/drive/MyDrive/ProteoGPT/" \
  --classifier_path "/content/drive/MyDrive/model.pt" \
  --output_path "/content/hnp4_variants_top_mask_1_million.csv" \
  --candidate_amp_path "/content/hnp4_variants_top_masks_1_million_modelpt_candidate_amps.csv" \
  --batch_size 256 \
  --num_workers 4 \
  --max_length 50

# Load CSV into DataFrame
import pandas as pd
df = pd.read_csv("hnp4_variants_top_mask_1_million.csv")

df_filtered_095 = df[df["Predicted Probabilities"] >= 0.95]
df_filtered_095.to_csv("/content/df_filtered_095.csv", index=False)

df_filtered_095

df_filtered_095.to_csv("hnp4_top_mask_1_million_095_filter.csv", index=False)
from google.colab import files

files.download("hnp4_top_mask_1_million_095_filter.csv")

######### Fasta to CSV ###########

from Bio import SeqIO
import pandas as pd

# === Input & output paths ===
fasta_path = "/content/variants_1M.fasta"
csv_path   = "/content/variants_1M.csv"

# === Parse FASTA and save as CSV ===
records = list(SeqIO.parse(fasta_path, "fasta"))
df = pd.DataFrame({
    "ID": [r.id for r in records],
    "Sequence": [str(r.seq) for r in records]
})
df.to_csv(csv_path, index=False)
print(f"‚úÖ Saved {len(df)} sequences to {csv_path}")

###### Creates chunks 5k for each fasta file.

import math
import os

# Ensure the filtered dataframe exists
assert "df_filtered_095" in globals(), "df_filtered does not exist!"

# Folder to save output
output_folder = "/content/fasta_chunks"
os.makedirs(output_folder, exist_ok=True)

# Convert to list of tuples
records = list(zip(df_filtered_095["ID"], df_filtered_095["Sequence"]))

chunk_size = 5000
num_chunks = math.ceil(len(records) / chunk_size)

print(f"Total sequences: {len(records)}")
print(f"Number of FASTA chunks: {num_chunks}")

# Create FASTA files
for i in range(num_chunks):
    chunk_records = records[i*chunk_size : (i+1)*chunk_size]
    fasta_path = f"{output_folder}/chunk_{i+1}.fasta"

    with open(fasta_path, "w") as f:
        for rec_id, seq in chunk_records:
            f.write(f">{rec_id}\n{seq}\n")

    print(f"Saved: {fasta_path}")

print("‚úì Done generating FASTA chunks!")

# Commented out IPython magic to ensure Python compatibility.
# %pip install --no-deps git+https://github.com/facebookresearch/esm@v0.3.1
# %pip install --no-deps biopython==1.83 PyYAML==6.0.1 h5py==3.11.0 click==8.1.7
# %pip install --no-deps scipy==1.13.1 matplotlib==3.8.4 pandas==2.2.2 numpy==2.1.1

# Commented out IPython magic to ensure Python compatibility.

###############################################
# 1. Unzip your standalone NetSurfP-3.0 package
###############################################

# Reset working directory -- prevents shell-init errors
# %cd /content

# Remove old extraction folder
!rm -rf NetSurfP3

# Unzip cleanly
!unzip -q "/content/drive/MyDrive/netsurfp-3.0.Linux.zip" -d NetSurfP3

!echo "‚úÖ Unzipped to /content/NetSurfP3"

# Commented out IPython magic to ensure Python compatibility.
###############################################
# 4. Install the NetSurfP-3.0 code itself
###############################################
import os

# find setup.py location (your folder structure)
base_path = "/content/NetSurfP3"
subfolder = "NetSurfP-3.0_standalone"

pkg_path = os.path.join(base_path, subfolder)

# %cd "$pkg_path"
!pip install .
!pip install -e .

import glob

fasta_files = glob.glob("/content/fasta_chunks/*.fasta")
num_chunks = len(fasta_files)

print("Number of FASTA files:", num_chunks)

import os

model_path = f"{pkg_path}/models/nsp3.pth"
input_dir = "/content/fasta_chunks"  # where your FASTA chunks are
output_dir = "/content/nsp3_test_output"

os.makedirs(output_dir, exist_ok=True)
num_chunks=15
# Loop over  n_chunks
for i in range(1, num_chunks+1):
    chunk_name = f"chunk_{i}.fasta"
    chunk_path = os.path.join(input_dir, chunk_name)
    if not os.path.exists(chunk_path):
        print(f"‚ùå Missing file: {chunk_path}, skipping...")
        continue

    # Each batch must have a batch ID (01 ... 100)
    batch_id = f"{i:02d}"
    print(f"üöÄ Running batch {batch_id} on: {chunk_path}")

    !python nsp3.py \
        -m "$model_path" \
        -i "$chunk_path" \
        -o "$output_dir" \
        -w "$batch_id" \
        -gpu 1

    print(f"‚úÖ Finished batch {batch_id}\n")

print("üéâ All batches complete!")

import json
import glob
import pandas as pd
import os

base_dir = "/content/nsp3_test_output"

# Match ONLY files like: /content/nsp3_test_output/01/01.json
# and automatically SKIP summary.json
json_files = sorted([
    p for p in glob.glob(f"{base_dir}/*/*.json")
    if os.path.basename(p) != "summary.json"
])

records = []

for json_path in json_files:

    # Load file
    try:
        with open(json_path) as f:
            data = json.load(f)
    except Exception as e:
        # Skip corrupt JSON files
        print(f"Skipping corrupt JSON: {json_path}")
        continue

    # Must be a list of prediction entries
    if not isinstance(data, list):
        continue

    for entry in data:

        # Valid predictions are dictionaries only
        if not isinstance(entry, dict):
            continue

        records.append({
            "desc": entry.get("desc", ""),
            "seq":  entry.get("seq", ""),
            "q8":   entry.get("q8", "")
        })

df = pd.DataFrame(records)
df

######## save df to csv


output_path = "/content/seq_q8_candidates_hnp4_top_mask_1M.csv"

# Save dataframe
df.to_csv(output_path, index=False)

# Download to your computer
from google.colab import files
files.download(output_path)

#### Upload AMPSorter_predictor_corrected.py to Colab

##### upload and continue overnight

# Commented out IPython magic to ensure Python compatibility.
# %cd /content

"""Secondary Structure Prediction."""

###############################################
# 5. Test GPU
###############################################
import torch
print("Torch version:", torch.__version__)
print("GPU available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("GPU:", torch.cuda.get_device_name(0))

"""**SOV refine calculation**

"""

import pandas as pd
from google.colab import files

# 1) Load CSVs
df1 = pd.read_csv("/content/seq_q8_candidates_hnp4_top_mask_1M.csv")               # contains "desc"
df2 = pd.read_csv("/content/hnp4_top_mask_1_million_095_filter.csv")        # contains "ID"

# 2) Merge df1.desc  with  df2.ID   (INNER JOIN)
merged_df = pd.merge(df1, df2, left_on="desc", right_on="ID", how="inner")

# 3) Save merged DataFrame
output_file = "variants_1M_variants_hnp4_prob_095.csv"
merged_df.to_csv(output_file, index=False)

# 4) Auto-download
files.download(output_file)

# Show dimensions
merged_df.shape

merged_df

##### upload csv file with data + perl script

# -------------------------
# Step 6: SOV_refine
# ------------------------
import pandas as pd
import numpy as np
import subprocess
import os

# -------------------------------
# Load your candidates CSV
# -------------------------------
df = pd.read_csv("Q8_variants_1M_variants_hnp4_prob_095.csv")
# -------------------------------
# WT Reference Q8 ‚Äî two options
# -------------------------------

# OPTION A: If WT exists inside the CSV
if "WT" in df["desc"].values:
    WT_q8 = df.loc[df["desc"] == "WT", "q8"].iloc[0]
else:
    # OPTION B: Manually define WT Q8 here
    WT_q8 = "CCECCSCCCCTTCEEEEEEEETTEEEEEEECCC"  # <-- your given example

print("WT Q8 reference:", WT_q8)

# ---------------------------------------
# Save WT reference Q8 into FASTA file
# ---------------------------------------
ref_fasta = "WT_q8_reference.fasta"

with open(ref_fasta, "w") as f:
    f.write(">WT\n" + WT_q8 + "\n")

# ---------------------------------------
# Helper: write predicted Q8 to FASTA
# ---------------------------------------
pred_fasta = "predicted_q8.fasta"

def write_pred_fasta(q8_string):
    with open(pred_fasta, "w") as f:
        f.write(">pred\n" + q8_string + "\n")

# ---------------------------------------
# Path to your uploaded SOV_refine script
# ---------------------------------------
perl_script = "/content/SOV_refine.pl"

# ---------------------------------------
# Calculate SOV_refine using Perl script
# ---------------------------------------
def calc_sov(q8_pred):
    if not isinstance(q8_pred, str):
        return np.nan

    # Save predicted Q8
    write_pred_fasta(q8_pred)

    # Run Perl script
    result = subprocess.run(
        ["perl", perl_script, ref_fasta, pred_fasta, "1"],
        capture_output=True,
        text=True
    ).stdout

    for line in result.splitlines():
        if line.startswith("SOV_refine\t"):
            try:
                return float(line.split("\t")[1])
            except:
                return np.nan

    return np.nan

# ---------------------------------------
# Apply SOV_refine to all rows
# ---------------------------------------
df["SOV_refine_score"] = df["q8"].apply(calc_sov)

# Save results + download
from google.colab import files
df.to_csv("seq_q8_candidates_with_sov.csv", index=False)
files.download("seq_q8_candidates_with_sov.csv")

print("‚úÖ Completed! Saved: seq_q8_candidates_with_sov.csv")
df.head()

import pandas as pd

# 1. Load CSV
df_2 = pd.read_csv("seq_q8_candidates_with_sov.csv")

# 2. Define the wild-type Q8 string
wt_q8 = "CCECCSCCCCTTCEEEEEEEETTEEEEEEECCC"

# 3. Function to compute identity percentage
def identity_percentage(seq, wt=wt_q8):
    matches = sum(a == b for a, b in zip(seq, wt))
    return (matches / len(wt)) * 100

# 4. Add identity percentage column
df_2["identity_pct_q8"] = df_2["q8"].apply(identity_percentage)

# 5. Sort:
#    - First by SOV_refine_score descending
#    - Then within same score by identity_pct descending
df_sorted = df_2.sort_values(
    by=["SOV_refine_score", "identity_pct_q8"],
    ascending=[False, False]
)

df_sorted

import numpy as np
import matplotlib.pyplot as plt

# --------- Extract values ----------
values = df_sorted['SOV_refine_score'].dropna().values

# --------- Statistics ----------
mean_val = np.mean(values)
median_val = np.median(values)

# --------- Plot ----------
plt.figure(figsize=(6, 4))

plt.hist(
    values,
    bins=30,
    edgecolor='black'
)

# Mean & median lines
plt.axvline(mean_val, linestyle='--', linewidth=1, color='black')
plt.axvline(median_val, linestyle='-', linewidth=1, color='black')

# --------- Annotation box ----------
textstr = (
    f"Mean   = {mean_val:.3f}\n"
    f"Median = {median_val:.3f}"
)

plt.text(
    0.02, 0.95, textstr,
    transform=plt.gca().transAxes,
    fontsize=10,
    verticalalignment='top',
    bbox=dict(boxstyle='round', facecolor='white', edgecolor='black')
)

# --------- Labels ----------
plt.xlabel("SOV_refine score")
plt.ylabel("Number of variants")
plt.title("Distribution of SOV_refine Scores")

plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt

# Extract values
values = df_sorted['SOV_refine_score'].dropna().values

# Sort values
x = np.sort(values)

# Empirical CDF
y = np.arange(1, len(x) + 1) / len(x)

# Plot
plt.figure(figsize=(6, 4))
plt.plot(x, y, linewidth=2)

plt.xlabel("SOV_refine score")
plt.ylabel("Cumulative probability")
plt.title("Empirical CDF of SOV_refine Scores")

plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt

# Extract values
values = df_sorted['SOV_refine_score'].dropna().values

# Sort values
x = np.sort(values)

# Empirical CDF
y = np.arange(1, len(x) + 1) / len(x)

threshold = 0.95
cdf_val = 0.91

plt.figure(figsize=(7.5, 4.5))
plt.plot(x, y, linewidth=2)

ax = plt.gca()

# ---- Axis limits ----
ax.set_xlim(-0.05, 1.05)
ax.set_ylim(-0.05, 1.05)

# ---- Explicit x-ticks (THIS is the key fix) ----
x_ticks = [0.0, 0.1, 0.2, 0.3,0.4, 0.5, 0.6, 0.7, 0.8, 0.8, 0.9, 0.9, 1.0]
ax.set_xticks(x_ticks)
ax.set_xticklabels([f"{t:.2f}" if t == 0.95 else f"{t:.1f}" for t in x_ticks])

# ---- Explicit y-ticks ----
y_ticks = [0.0, 0.2, 0.4, 0.6, 0.8, cdf_val, 1.0]
ax.set_yticks(y_ticks)
ax.set_yticklabels([f"{t:.2f}" if t == cdf_val else f"{t:.1f}" for t in y_ticks])

# ---- Brown dashed guide lines ----
ax.plot([threshold, threshold], [0, cdf_val],
        linestyle='--', color='brown', linewidth=1.5)

ax.plot([ax.get_xlim()[0], threshold], [cdf_val, cdf_val],
        linestyle='--', color='brown', linewidth=1.5)

# ---- Annotation box outside plot ----
ax.text(
    1.04, 0.5,
    f"CDF(0.95) = {cdf_val:.2f}",
    transform=ax.transAxes,
    fontsize=9,
    va='center',
    ha='left',
    bbox=dict(boxstyle='round', facecolor='white', edgecolor='black')
)

plt.xlabel("SOV_refine score")
plt.ylabel("Cumulative probability")
plt.title("Empirical CDF of SOV_refine Scores")

plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle, Polygon
from pathlib import Path

# ========= ADD THIS LINE ONLY =========
WT_Q8 = "CCECCSCCCCTTCEEEEEEEETTEEEEEEECCC"
L = len(WT_Q8)
sov_threshold=0
# ========= LOAD CSV =========
csv_path = "seq_q8_candidates_with_sov.csv"   # <-- update path
fdf = pd.read_csv(csv_path)

# Ensure q8 column exists
assert "q8" in fdf.columns, "CSV must contain a 'q8' column"

# ========= Build heatmap for filtered variants only =========
if len(fdf) == 0:
    print("\nNo variants passed the threshold; skipping heatmap.")
else:
    # Prepare Q8 list (aligned/padded to WT)
    q8_seqs = fdf["q8"].dropna().astype(str).tolist()
    q8_seqs = [seq[:L].ljust(L, "-") for seq in q8_seqs]

    # Q8 states & mapping (as in your earlier figure)
    q8_states = list("HGIETBSTC")
    state_to_idx = {s: i for i, s in enumerate(q8_states)}

    # Encode to indices
    encoded = np.array([[state_to_idx.get(aa, -1) for aa in seq] for seq in q8_seqs], dtype=int)

    # Frequency matrix
    freq_matrix = np.zeros((len(q8_states), L), dtype=float)
    for i, _state in enumerate(q8_states):
        freq_matrix[i] = (encoded == i).sum(axis=0)
    freq_matrix /= max(len(q8_seqs), 1)

    # Helper: draw a uniform-color E-arrow on WT strip
    def draw_uniform_arrow(ax, x_start, x_end, y=0.5, bar_height=0.30, color="#5b2ca0"):
        length = max(0.001, x_end - x_start)
        head_len = max(0.8, 0.12 * length)
        body_end = x_end - head_len
        ax.add_patch(Rectangle((x_start, y - bar_height/2),
                               max(0.0, body_end - x_start),
                               bar_height,
                               color=color, linewidth=0))
        head = np.array([
            [body_end,      y - bar_height/2],
            [x_end,         y],
            [body_end,      y + bar_height/2],
        ])
        ax.add_patch(Polygon(head, closed=True, color=color, linewidth=0))

    # Figure
    fig = plt.figure(figsize=(16, 5))
    ax = fig.add_axes([0.06, 0.40, 0.88, 0.52])

    im = ax.imshow(freq_matrix, aspect="auto", cmap="Spectral", origin="upper",
                   extent=[0.5, L+0.5, len(q8_states)+0.5, 0.5])

    ax.set_xticks(np.arange(1, L+1))
    ax.set_xticklabels(list(WT_Q8), rotation=0)
    ax.set_yticks(np.arange(1, len(q8_states)+1))
    ax.set_yticklabels(q8_states)
    ax.set_xlabel("Residue Position (Aligned to WT HNP-4)")
    ax.set_ylabel("Q8 State")
    ax.set_title(f"Q8 Conservation Heatmap ‚Äî Variants with SOV_refine ‚â• {sov_threshold}")

    cbar = fig.colorbar(im, ax=ax, fraction=0.025, pad=0.02)
    cbar.set_label("Q8 state frequency")

    # WT strip
    strip_ax = fig.add_axes([0.06, 0.18, 0.88, 0.10], sharex=ax)
    strip_ax.set_xlim(0.5, L+0.5)
    strip_ax.set_ylim(0, 1)
    strip_ax.axis("off")
    strip_ax.plot([1, L], [0.5, 0.5], color="black", linewidth=2)

    # Draw Œ≤-strand (E) runs as uniform deep-purple arrows
    in_E = False
    start = None
    for i, ch in enumerate(WT_Q8 + " ", start=1):
        if ch == "E" and not in_E:
            in_E = True
            start = i
        elif ch != "E" and in_E:
            in_E = False
            end = i - 1
            draw_uniform_arrow(strip_ax, start, end + 0.1, y=0.5, bar_height=0.30, color="#5b2ca0")

    out_fig = Path("Q8_heatmap_filtered_0.95.png")
    fig.savefig(out_fig, dpi=300, bbox_inches="tight")
    plt.show()
    print(f"\n‚úì Heatmap saved to: {out_fig.resolve()}")

df_sov_filtered_095 = df_sorted[df_sorted["SOV_refine_score"] >= 0.95]
df_sov_filtered_095

# ---- Convert df_sov_filtered_095 to FASTA ----
output_fasta = "df_sov_filtered_095_hnp4.fasta"

with open(output_fasta, "w") as f:
    for _, row in df_sov_filtered_095.iterrows():
        header = str(row["desc"]).strip()
        sequence = str(row["seq"]).strip()

        # write to fasta
        f.write(f">{header}\n{sequence}\n")

print("FASTA saved to:", output_fasta)

df_sov_filtered_095.to_csv("df_sov_filtered_095_hnp4.csv", index=False)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle, Polygon
from pathlib import Path

# ========= ADD THIS LINE ONLY =========
WT_Q8 = "CCECCSCCCCTTCEEEEEEEETTEEEEEEECCC"
L = len(WT_Q8)
sov_threshold=0.95
# ========= LOAD CSV =========
csv_path = "df_sov_filtered_095_hnp4.csv"   # <-- update path
fdf = pd.read_csv(csv_path)

# Ensure q8 column exists
assert "q8" in fdf.columns, "CSV must contain a 'q8' column"

# ========= Build heatmap for filtered variants only =========
if len(fdf) == 0:
    print("\nNo variants passed the threshold; skipping heatmap.")
else:
    # Prepare Q8 list (aligned/padded to WT)
    q8_seqs = fdf["q8"].dropna().astype(str).tolist()
    q8_seqs = [seq[:L].ljust(L, "-") for seq in q8_seqs]

    # Q8 states & mapping (as in your earlier figure)
    q8_states = list("HGIETBSTC")
    state_to_idx = {s: i for i, s in enumerate(q8_states)}

    # Encode to indices
    encoded = np.array([[state_to_idx.get(aa, -1) for aa in seq] for seq in q8_seqs], dtype=int)

    # Frequency matrix
    freq_matrix = np.zeros((len(q8_states), L), dtype=float)
    for i, _state in enumerate(q8_states):
        freq_matrix[i] = (encoded == i).sum(axis=0)
    freq_matrix /= max(len(q8_seqs), 1)

    # Helper: draw a uniform-color E-arrow on WT strip
    def draw_uniform_arrow(ax, x_start, x_end, y=0.5, bar_height=0.30, color="#5b2ca0"):
        length = max(0.001, x_end - x_start)
        head_len = max(0.8, 0.12 * length)
        body_end = x_end - head_len
        ax.add_patch(Rectangle((x_start, y - bar_height/2),
                               max(0.0, body_end - x_start),
                               bar_height,
                               color=color, linewidth=0))
        head = np.array([
            [body_end,      y - bar_height/2],
            [x_end,         y],
            [body_end,      y + bar_height/2],
        ])
        ax.add_patch(Polygon(head, closed=True, color=color, linewidth=0))

    # Figure
    fig = plt.figure(figsize=(16, 5))
    ax = fig.add_axes([0.06, 0.40, 0.88, 0.52])

    im = ax.imshow(freq_matrix, aspect="auto", cmap="Spectral", origin="upper",
                   extent=[0.5, L+0.5, len(q8_states)+0.5, 0.5])

    ax.set_xticks(np.arange(1, L+1))
    ax.set_xticklabels(list(WT_Q8), rotation=0)
    ax.set_yticks(np.arange(1, len(q8_states)+1))
    ax.set_yticklabels(q8_states)
    ax.set_xlabel("Residue Position (Aligned to WT HNP-4)")
    ax.set_ylabel("Q8 State")
    ax.set_title(f"Q8 Conservation Heatmap ‚Äî Variants with SOV_refine ‚â• {sov_threshold}")

    cbar = fig.colorbar(im, ax=ax, fraction=0.025, pad=0.02)
    cbar.set_label("Q8 state frequency")

    # WT strip
    strip_ax = fig.add_axes([0.06, 0.18, 0.88, 0.10], sharex=ax)
    strip_ax.set_xlim(0.5, L+0.5)
    strip_ax.set_ylim(0, 1)
    strip_ax.axis("off")
    strip_ax.plot([1, L], [0.5, 0.5], color="black", linewidth=2)

    # Draw Œ≤-strand (E) runs as uniform deep-purple arrows
    in_E = False
    start = None
    for i, ch in enumerate(WT_Q8 + " ", start=1):
        if ch == "E" and not in_E:
            in_E = True
            start = i
        elif ch != "E" and in_E:
            in_E = False
            end = i - 1
            draw_uniform_arrow(strip_ax, start, end + 0.1, y=0.5, bar_height=0.30, color="#5b2ca0")

    out_fig = Path("Q8_heatmap_filtered_0.95.png")
    fig.savefig(out_fig, dpi=300, bbox_inches="tight")
    plt.show()
    print(f"\n‚úì Heatmap saved to: {out_fig.resolve()}")

"""Setup Tango"""

#### --- CELL 1: SETUP (Drive, Tango single-file, helpers) ---

import os
import pandas as pd
from Bio import SeqIO, SeqRecord
from Bio.Seq import Seq

# -------------------------------------------------------------
# 1) Mount drive
# -------------------------------------------------------------
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# -------------------------------------------------------------
# 2) Tango binary is a SINGLE FILE, not a folder
# -------------------------------------------------------------
drive_tango_binary = "/content/drive/MyDrive/tango_x86_64_release"
local_tango_binary = "/content/tango_x86_64_release"

# Copy binary into Colab
print("üìÅ Copying Tango binary from Drive...")
!cp "{drive_tango_binary}" "{local_tango_binary}"

# Make it executable
!chmod +x "{local_tango_binary}"

tango_binary = local_tango_binary
print("‚úÖ Tango binary:", tango_binary)

# -------------------------------------------------------------
# 3) normalize_desc helper
# -------------------------------------------------------------
def normalize_desc(x: str) -> str:
    return x.strip().replace("|", "_").replace(" ", "_").lower()

# -------------------------------------------------------------
# 4) aggregation detector
# -------------------------------------------------------------
def has_aggregating_segment(fp, threshold=5.0, window_sizes=(5, 6)):
    vals = []
    with open(fp) as f:
        lines = f.read().splitlines()[1:]
    for ln in lines:
        parts = ln.strip().split()
        try:
            vals.append(float(parts[5]))
        except:
            continue
    if not vals:
        return False

    s = pd.Series(vals)
    for w in window_sizes:
        for i in range(len(s) - w + 1):
            if (s[i:i+w] >= threshold).all():
                return True
    return False

# -------------------------------------------------------------
# 5) run_tango wrapper
# -------------------------------------------------------------
def run_tango(seq_id, sequence, out_dir="outputs"):
    os.makedirs(out_dir, exist_ok=True)

    safe = seq_id.replace("|","_").replace(" ","_")
    out_txt = os.path.join(out_dir, f"{safe}.txt")
    out_raw = os.path.join(out_dir, f"{safe}.out")

    cmd = (
        f'"{tango_binary}" {safe} '
        f'ct="N" nt="N" ph="7.0" te="310.15" io="0.165" seq="{sequence}" > "{out_raw}"'
    )
    os.system(cmd)

    raw_txt = f"{safe}.txt"
    if not os.path.exists(raw_txt):
        print(f"[‚ö†Ô∏è WARNING] No Tango txt output for {safe}.")
        return None, False

    os.replace(raw_txt, out_txt)
    passed = not has_aggregating_segment(out_txt)
    return out_txt, passed

print("üü¢ Setup complete ‚Äî Tango is ready.")

"""Setup Protein-sol

"""

from google.colab import drive
import os

# 1Ô∏è‚É£ Mount Drive
drive.mount('/content/drive', force_remount=True)

# 2Ô∏è‚É£ Path to your Protein-Sol folder
protein_sol_dir = "/content/drive/MyDrive/Protein-Sol"

print(f"üìÅ Using Protein-Sol directory: {protein_sol_dir}")

print("""
Required files in this folder:
------------------------------------------
1) multiple_prediction_wrapper_export.sh
2) fasta_seq_reformat_export.pl
3) seq_compositions_perc_pipeline_export.pl
4) seq_props_ALL_export.pl
5) profiles_gather_export.pl
6) server_prediction_seq_export.pl
7) seq_reference_data.txt
------------------------------------------
""")

# 3Ô∏è‚É£ Show existing files (optional)
print("üìÑ Files found:")
for f in os.listdir(protein_sol_dir):
    print("   -", f)

# 4Ô∏è‚É£ Make all scripts executable
print("\nüîß Setting permissions...")
for fname in os.listdir(protein_sol_dir):
    if fname.endswith(".sh") or fname.endswith(".pl"):
        fullpath = os.path.join(protein_sol_dir, fname)
        !chmod +x "{fullpath}"
        print(f"   ‚úî chmod +x {fname}")

print("\n‚úÖ Protein-Sol is ready to use!")

"""
 #### --- STEP 1: TANGO AGGREGATION FILTERING ---

"""

#### --- STEP 1: TANGO AGGREGATION FILTERING ---

tango_results = []

for _, row in df_sov_filtered_095.iterrows():
    seq_id = str(row["desc"])
    sequence = str(row["seq"])

    txt_path, passed = run_tango(seq_id, sequence)

    tango_results.append({
        "desc": seq_id,
        "Tango_Passed": 1 if passed else 0,
        "Tango_txt": txt_path
    })

tango_df = pd.DataFrame(tango_results)

print("Total sequences processed:", len(tango_df))
print("Passed Tango:", tango_df['Tango_Passed'].sum())

# Keep only sequences that passed aggregation filter
tango_pass_df = tango_df[tango_df["Tango_Passed"] == 1].copy()

tango_pass_df

#### --- STEP 2: WRITE FASTA FOR PROTEIN-SOL ---

from Bio.Seq import Seq
from Bio.SeqRecord import SeqRecord
from Bio import SeqIO

fasta_file = "protein_sol_input.fasta"

records = []
for _, row in df_sov_filtered_095.iterrows():
    seq_id = row["desc"]
    sequence = row["seq"]
    records.append(SeqRecord(Seq(sequence), id=seq_id, description=""))

SeqIO.write(records, fasta_file, "fasta")
print("FASTA file written:", fasta_file)
print("Number of sequences:", len(records))

#### --- STEP 3: RUN PROTEIN-SOL ---

protein_sol_dir = "/content/drive/MyDrive/Protein-Sol"

# Copy tools into current workspace
!cp {protein_sol_dir}/* .

# Make wrapper executable
!chmod +x multiple_prediction_wrapper_export.sh

# Run Protein-Sol
!./multiple_prediction_wrapper_export.sh protein_sol_input.fasta

#### --- STEP 4: PARSE PROTEIN-SOL OUTPUT ---

sol_results = []
with open("seq_prediction.txt") as f:
    for line in f:
        # Lines look like:
        # SEQUENCE PREDICTIONS, >ID, ..., solubility, ..., pI
        if line.upper().startswith("SEQUENCE PREDICTIONS"):
            parts = [p.strip() for p in line.split(",")]
            desc = parts[1].lstrip(">")
            solubility = float(parts[3])
            pI = float(parts[5])

            sol_results.append((desc, solubility, pI))

sol_df = pd.DataFrame(sol_results, columns=["desc", "Scaled_Solubility", "pI"])
sol_df["desc"] = sol_df["desc"].str.lower()  # unify for merging

print("Protein-Sol predictions:", len(sol_df))
sol_df.head()

#### --- STEP 5 + STEP 6: MERGE ALL FILTERS TOGETHER ---

# Ensure real copies to avoid warnings
df_sov_filtered_095 = df_sov_filtered_095.copy()
tango_pass_df = tango_pass_df.copy()
sol_df = sol_df.copy()

# Normalize desc for merging
df_sov_filtered_095["desc_norm"] = df_sov_filtered_095["desc"].str.lower()
tango_pass_df["desc_norm"] = tango_pass_df["desc"].str.lower()
sol_df["desc_norm"] = sol_df["desc"].str.lower()

# Merge SOV + identity + Tango
merged = pd.merge(
    df_sov_filtered_095,
    tango_pass_df[["desc_norm", "Tango_Passed"]],
    on="desc_norm",
    how="inner"
)

# Merge with Protein-Sol (solubility + pI)
final_df = pd.merge(
    merged,
    sol_df[["desc_norm", "Scaled_Solubility", "pI"]],
    on="desc_norm",
    how="inner"
)

# -------------------------------------------
# APPLY the solubility filter HERE
# -------------------------------------------
sol_threshold = 0.45
final_df = final_df[ final_df["Scaled_Solubility"] >= sol_threshold ].copy()
final_df.reset_index(drop=True, inplace=True)

# -------------------------------------------
# ADD sequence identity vs query_seq (positional)
# -------------------------------------------
query_seq = "VCSCRLVFCRRTELRVGNCLIGGVSFTYCCTRV"

def seq_identity(seq, ref):
    matches = sum(a == b for a, b in zip(seq, ref))
    return (matches / len(ref)) * 100

final_df["identity_pct_seq"] = final_df["seq"].apply(lambda s: seq_identity(s, query_seq))


# Sort final candidates
final_df = final_df.sort_values(
    by=["SOV_refine_score", "identity_pct_q8"],
    ascending=[False, False]
).reset_index(drop=True)

print("Final sequences after ALL filters (including solubility):", len(final_df))
final_df.head()

# ---- FINAL SUMMARY ----
total_screened = 1_000_000   # your full screening size
final_count = len(final_df)

percentage = (final_count / total_screened) * 100

print(f"Final variants after ALL filters: {final_count}")
print(f"Percentage of functional variants out of 1 million: {percentage:.4f}%")

import pandas as pd
from google.colab import files

##### ---------------------------------------------------------------
##### 1) Load or use your final dataframe
#####    Make sure "final_df_filtered" is already defined in your notebook
##### ---------------------------------------------------------------

df = final_df.copy()

##### ---------------------------------------------------------------
##### 2) Save the dataframe as CSV
#####    This will allow you to download the exact filtered table
##### ---------------------------------------------------------------

csv_path = "all_final_df_filtered_hnp4.csv"
df.to_csv(csv_path, index=False)

##### ---------------------------------------------------------------
##### 3) Create FASTA file
#####    >desc  ‚Üê header from "desc"
#####    sequence  ‚Üê from "seq"
##### ---------------------------------------------------------------

fasta_path = "all_final_df_filtered_hnp4.fasta"

with open(fasta_path, "w") as f:
    for _, row in df.iterrows():
        f.write(f">{row['desc']}\n{row['seq']}\n")

##### ---------------------------------------------------------------
##### 4) Download both files to your local computer
##### ---------------------------------------------------------------

files.download(csv_path)
files.download(fasta_path)

print("‚úì CSV and FASTA files created and downloaded.")

##### upload selected filtered fasta files and predict structres the TM align

!pip install esm

from esm.sdk import client
from esm.sdk.api import ESMProtein, GenerationConfig

model = client(
    "esm3-medium-2024-08",
    token="1ztosIDJe6IZQVYbDUWjg1"
)

# Commented out IPython magic to ensure Python compatibility.
# #@title ##run **ESMFold** (FASTA Input & Optimized for PDB Generation)
# %%time
# from string import ascii_uppercase, ascii_lowercase
# import hashlib, re, os, time
# import numpy as np
# import torch
# from jax.tree_util import tree_map
# import gc
# from Bio import SeqIO # Import SeqIO for FASTA parsing
# 
# # --- 1. INSTALLATION AND MODEL DEFINITION (Integrated) ---
# version = "1" # Set model version
# model_name = "esmfold_v0.model" if version == "0" else "esmfold.model"
# model_name_ = None # Placeholder for current loaded model
# 
# if not os.path.isfile(model_name):
#   # download esmfold params
#   print("Starting file download...")
#   os.system("apt-get install aria2 -qq")
#   os.system(f"aria2c -q -x 16 https://colabfold.steineggerlab.workers.dev/esm/{model_name} &")
# 
# if not os.path.isfile("finished_install"):
#   # install libs
#   print("installing libs...")
#   # Ensure biopython (for SeqIO) is installed
#   os.system("pip install -q omegaconf pytorch_lightning biopython ml_collections einops py3Dmol modelcif")
#   os.system("pip install -q git+https://github.com/NVIDIA/dllogger.git")
# 
#   print("installing openfold...")
#   os.system(f"pip install -q git+https://github.com/sokrypton/openfold.git")
# 
#   print("installing esmfold...")
#   os.system(f"pip install -q git+https://github.com/sokrypton/esm.git")
#   os.system("touch finished_install")
# 
# # wait for Params to finish downloading...
# while not os.path.isfile(model_name):
#   time.sleep(5)
#   if os.path.isfile(f"{model_name}.aria2"):
#     print("downloading params...")
#     while os.path.isfile(f"{model_name}.aria2"):
#       time.sleep(5)
# 
# # --- Helper Functions ---
# def get_hash(x): return hashlib.sha1(x.encode()).hexdigest()
# 
# def get_sequences_from_fasta(fasta_path):
#     """Parses sequences from a FASTA file using Biopython."""
#     sequences = []
#     ids = []
#     try:
#         for record in SeqIO.parse(fasta_path, "fasta"):
#             sequences.append(str(record.seq).upper())
#             ids.append(str(record.id))
#         return sequences, ids
#     except FileNotFoundError:
#         print(f"Error: FASTA file not found at {fasta_path}")
#         return [], []
#     except Exception as e:
#         print(f"Error reading FASTA file: {e}")
#         return [], []
# 
# 
# # --- USER INPUT PARAMETERS (No Form Fields) ---
# jobname = "ESM_PDB_Output_FASTA" # Output folder name
# jobname = re.sub(r'\W+', '', jobname)[:50]
# 
# # **CHANGE THIS LINE to match the name of your uploaded FASTA file**
# FASTA_FILE_PATH = "all_final_df_filtered_hnp4.fasta"
# 
# num_recycles = 3
# chain_linker = 25
# copies_per_sequence = 1 # Keep this at 1 for monomer prediction
# 
# # --- Preparation before the loop ---
# # Get sequences and IDs from the FASTA file
# sequences_to_predict, sequence_ids = get_sequences_from_fasta(FASTA_FILE_PATH)
# 
# if not sequences_to_predict:
#   print("Error: No sequences were loaded or the file was not found.")
# else:
#   print(f"\nSuccessfully loaded {len(sequences_to_predict)} sequence(s) from {FASTA_FILE_PATH}.")
# 
#   # Create the single main output directory
#   OUTPUT_DIR = jobname
#   os.makedirs(OUTPUT_DIR, exist_ok=True)
#   print(f"PDB files will be saved in: /{OUTPUT_DIR}")
# 
#   # --- Model Loading ---
#   if "model" not in dir() or model_name != model_name_:
#     if "model" in dir():
#       del model
#       gc.collect()
#       if torch.cuda.is_available():
#         torch.cuda.empty_cache()
# 
#     try:
#       model = torch.load(model_name, weights_only=False)
#       model.eval().cuda().requires_grad_(False)
#       model_name_ = model_name
#     except Exception as e:
#       print(f"FATAL ERROR loading model: {e}")
#       sequences_to_predict = [] # Clear list to stop execution
# 
#   # --- MAIN PREDICTION LOOP ---
#   for i, input_sequence in enumerate(sequences_to_predict):
# 
#     # Use the ID from the FASTA file for the filename
#     current_id = sequence_ids[i]
# 
#     # 1. Process the current sequence
#     sequence = input_sequence.upper()
#     # Cleaning the sequence (same as previous scripts)
#     sequence = re.sub("[^A-Z:]", "", sequence.replace("/",":"))
#     sequence = re.sub(":+",":",sequence)
#     sequence = re.sub("^[:]+","",sequence)
#     sequence = re.sub("[:]+$","",sequence)
# 
#     sequence = ":".join([sequence] * copies_per_sequence)
# 
#     # 2. Sequence-specific variables
#     seqs = sequence.split(":")
#     length = sum([len(s) for s in seqs])
# 
#     # Generate descriptive filename using FASTA ID
#     file_id = f"{current_id}_len{length}_h{get_hash(sequence)[:5]}"
#     print(f"\nProcessing {file_id}. Length: {length}.")
# 
#     # 3. Adjust chunk size
#     if length > 700:
#       model.set_chunk_size(64)
#     else:
#       model.set_chunk_size(128)
#     torch.cuda.empty_cache()
# 
#     # 4. **CORE PREDICTION STEP**
#     output = model.infer(sequence,
#                          num_recycles=num_recycles,
#                          chain_linker="X"*chain_linker,
#                          residue_index_offset=512)
# 
#     # 5. **OUTPUT PROCESSING AND SAVING**
#     pdb_str = model.output_to_pdb(output)[0]
#     output = tree_map(lambda x: x.cpu().numpy(), output)
#     ptm = output["ptm"][0]
#     plddt = output["plddt"][0,...,1].mean()
# 
#     print(f'  ptm: {ptm:.3f} plddt: {plddt:.3f}')
# 
#     # Save ONLY the PDB file to the common output directory
#     filename = os.path.join(OUTPUT_DIR, f"{file_id}_ptm{ptm:.3f}_r{num_recycles}.pdb")
#     with open(filename,"w") as out:
#         out.write(pdb_str)
# 
#     # 6. Clean up GPU memory
#     del output
#     gc.collect()
#     if torch.cuda.is_available():
#         torch.cuda.empty_cache()
# 
#   print("\n--- All PDB predictions complete. ---")



#@title download predictions
from google.colab import files
import shutil
import os

# Get the directory name from the previous cell's input
jobname = "ESM_PDB_Output" # Must match the jobname set in the prediction cell

# Create the zip file for the entire output directory
zip_filename = f'{jobname}.zip'
shutil.make_archive(jobname, 'zip', jobname)

# Download the zip file
print(f"Downloading {zip_filename}...")
files.download(zip_filename)

# Clean up local files (optional)
# shutil.rmtree(jobname)
# os.remove(zip_filename)

##### TM align calculation:

!pip install tmtools biopython --quiet

# =============================================================
#   TM-align ALL ESM3-generated structures vs. HNP-4 (1ZMM)
#   Using tmtools.tm_align + tmtools.io.get_residue_data
#   One complete Colab cell
# =============================================================

import os
import numpy as np
import pandas as pd
from copy import deepcopy
from Bio.PDB import PDBParser, PDBList, StructureBuilder, PDBIO
from google.colab import files

# ---- TM-align official API ----
from tmtools import tm_align
from tmtools.io import get_residue_data


# ---- Helper: extract first chain ----
def first_chain(structure):
    return next(structure.get_chains())


# ==================== 1) Load Reference (HNP-4) PDB ID: 1ZMM ====================

REFERENCE_PDB_ID = "1ZMM"

pdbl = PDBList()
ref_path = pdbl.retrieve_pdb_file(REFERENCE_PDB_ID, pdir=".", file_format="pdb")

pdb_parser = PDBParser(QUIET=True)
ref_structure = pdb_parser.get_structure("ref", ref_path)
target_chain = first_chain(ref_structure)

coords_tar, seq_tar = get_residue_data(target_chain)

print(f"Loaded reference structure: {REFERENCE_PDB_ID}")
print("Reference length:", len(seq_tar))


# ==================== 2) Folder of generated structures ====================

gen_folder = "/content/ESM_PDB_Output_FASTA"
pdb_files = sorted(f for f in os.listdir(gen_folder) if f.endswith(".pdb"))

print(f"Found {len(pdb_files)} generated PDB files.")


results = []


# ==================== 3) Main TM-align Loop ====================

for pdb_file in pdb_files:

    mobile_path = os.path.join(gen_folder, pdb_file)
    mob_structure = pdb_parser.get_structure("mob", mobile_path)
    mobile_chain = first_chain(mob_structure)

    # Extract AA sequence + CA coords
    coords_mob, seq_mob = get_residue_data(mobile_chain)

    # Run TM-align
    result = tm_align(coords_tar, coords_mob, seq_tar, seq_mob)

    tm_score = result.tm_norm_chain1
    rmsd = result.rmsd
    U = result.u   # rotation
    t = result.t   # translation

    print(f"\n=== {pdb_file} ===")
    print("TM-score:", tm_score)
    print("RMSD:", rmsd)

    # ==================== Build aligned overlay PDB ====================

    mobile_aligned = deepcopy(mobile_chain)
    for atom in mobile_aligned.get_atoms():
        atom.coord = np.dot(U, atom.coord) + t

    sb = StructureBuilder.StructureBuilder()
    sb.init_structure("aligned")
    sb.init_model(0)

    # chain A = reference
    sb.init_chain("A")
    for r in target_chain.get_residues():
        sb.structure[0]["A"].add(deepcopy(r))

    # chain B = aligned model
    sb.init_chain("B")
    for r in mobile_aligned.get_residues():
        sb.structure[0]["B"].add(deepcopy(r))

    out_pdb = os.path.join(gen_folder, pdb_file.replace(".pdb","") + "_aligned.pdb")
    io = PDBIO()
    io.set_structure(sb.structure)
    io.save(out_pdb)

    # ==================== Collect Results ====================
    results.append({
        "file": pdb_file,
        "tm_score": float(tm_score),
        "rmsd": float(rmsd),
        "seq_len": len(seq_mob)
    })


# ==================== 4) Save summary CSV ====================

df = pd.DataFrame(results)
csv_path = "/content/tm_results.csv"
df.to_csv(csv_path, index=False)

print("\nSaved summary CSV:", csv_path)
files.download(csv_path)

print("\nDone.")

import pandas as pd

fasta_path = "/content/all_final_df_filtered_hnp4.fasta"

records = []
cur_id = None
seq_chunks = []

with open(fasta_path, "r") as f:
    for line in f:
        line = line.strip()
        if line.startswith(">"):
            if cur_id is not None:
                records.append({"ID": cur_id, "Sequence": "".join(seq_chunks)})
            cur_id = line[1:].strip()
            seq_chunks = []
        else:
            seq_chunks.append(line)

# last entry
if cur_id:
    records.append({"ID": cur_id, "Sequence": "".join(seq_chunks)})

df_seq = pd.DataFrame(records)
print("Loaded sequences:", df_seq.shape)

import pandas as pd
import numpy as np
import pandas as pd

df1 = pd.read_csv("all_final_df_filtered_hnp4.csv")

print(df1.shape)
df1.head()

df2 = pd.read_csv("tm_results_1ZMM.csv")
df2 = df2[df2["tm_score"] >= 0.6].reset_index(drop=True)
print(df2.shape)
df2["file"] = df2["file"].str.split("_len33").str[0]
df2.head()

df_workflow_final_output = df1.merge(
    df2,
    left_on="desc",
    right_on="file",
    how="inner"
)

print("df1 rows:", len(df1))
print("df2 rows:", len(df2))
print("merged rows:", len(df_workflow_final_output))

df_workflow_final_output.head()

cols_to_drop = ["seq", "file", "ID"]
df_workflow_final_output = df_workflow_final_output.drop(
    columns=[c for c in cols_to_drop if c in df_workflow_final_output.columns]
)

ordered_cols = [
    "desc",
    "Sequence",
    "q8",
    "Predicted Labels",
    "Predicted Probabilities",
    "SOV_refine_score",
    "identity_pct_q8",
    "identity_pct_seq",
    "Tango_Passed",
    "Scaled_Solubility",
    "pI",
    "tm_score",
    "rmsd",
    "seq_len"
]

df_workflow_final_output = df_workflow_final_output[ordered_cols]

df_workflow_final_output = df_workflow_final_output.rename(
    columns={"desc": "ID"}
)

df_workflow_final_output

output_path = "/content/df_workflow_final_output_final.csv"
df_workflow_final_output.to_csv(output_path, index=False)

from google.colab import files
files.download(output_path)

########### Shannon entropy per sequence position

import numpy as np
import pandas as pd
from collections import Counter

sequences = df_workflow_final_output["Sequence"].values

# Sanity check
lengths = {len(seq) for seq in sequences}
assert len(lengths) == 1, f"Sequences have different lengths: {lengths}"

L = lengths.pop()
N = len(sequences)

entropy = []

for pos in range(L):
    residues = [seq[pos] for seq in sequences]
    counts = Counter(residues)

    H = 0.0
    for count in counts.values():
        p = count / N
        H -= p * np.log2(p)

    entropy.append(H)

entropy_df = pd.DataFrame({
    "position": range(L),
    "shannon_entropy_bits": entropy
})

entropy_df.head()

import matplotlib.pyplot as plt

plt.figure(figsize=(16, 4))
plt.bar(
    entropy_df["position"],
    entropy_df["shannon_entropy_bits"]
)

plt.xlabel("Sequence position")
plt.ylabel("Shannon entropy (bits)")
plt.title("Positional Shannon Entropy Across Designed Sequences")

plt.tight_layout()
plt.show()

import numpy as np

# Example: put YOUR fixed positions here (0-based indices)
fixed_positions = {1, 3, 4, 8, 9, 12, 14, 16, 18, 22, 24, 27, 28, 29, 30, 31, 32}  # <-- edit as needed

L = int(entropy_df["position"].max() + 1)
unfixed_positions = sorted(set(range(L)) - set(fixed_positions))

mean_entropy_unfixed_bits = entropy_df.loc[
    entropy_df["position"].isin(unfixed_positions),
    "shannon_entropy_bits"
].mean()

# If your design alphabet is 11 AAs, normalize by log2(11)
Hmax_11 = np.log2(11)
mean_entropy_unfixed_norm = (entropy_df.loc[
    entropy_df["position"].isin(unfixed_positions),
    "shannon_entropy_bits"
] / Hmax_11).mean()

print("Unfixed positions:", unfixed_positions)
print(f"Mean entropy (unfixed) = {mean_entropy_unfixed_bits:.4f} bits")
print(f"Mean normalized entropy (unfixed, /log2(11)) = {mean_entropy_unfixed_norm:.4f}")