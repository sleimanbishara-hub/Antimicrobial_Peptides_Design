# -*- coding: utf-8 -*-
"""WorkFlow_One_Function.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-ny_Ye-wUmnQFzy5qcYE8prniUwMUMO_
"""

!pip install --no-deps git+https://github.com/facebookresearch/esm@v0.3.1
!pip install --no-deps biopython==1.83 PyYAML==6.0.1 h5py==3.11.0 click==8.1.7
!pip install --no-deps scipy==1.13.1 matplotlib==3.8.4 pandas==2.2.2 numpy==2.1.1

!pip install pymsaviz

!sudo apt install clustalo

!apt-get update
!apt-get install -y ncbi-blast+

from google.colab import drive
drive.mount('/content/drive')

!pip install -r /content/drive/MyDrive/requirements.txt

!makeblastdb \
  -in /content/drive/MyDrive/AMPDB_Master_Dataset.fasta \
  -dbtype prot \
  -out /content/ampdb_v1_db

# Commented out IPython magic to ensure Python compatibility.

###############################################
# 1. Unzip your standalone NetSurfP-3.0 package
###############################################

# Reset working directory -- prevents shell-init errors
# %cd /content

# Remove old extraction folder
!rm -rf NetSurfP3

# Unzip cleanly
!unzip -q "/content/drive/MyDrive/netsurfp-3.0.Linux.zip" -d NetSurfP3

!echo "âœ… Unzipped to /content/NetSurfP3"

# Commented out IPython magic to ensure Python compatibility.
###############################################
# 4. Install the NetSurfP-3.0 code itself
###############################################
import os

# find setup.py location (your folder structure)
base_path = "/content/NetSurfP3"
subfolder = "NetSurfP-3.0_standalone"

pkg_path = os.path.join(base_path, subfolder)

# %cd "$pkg_path"
!pip install .
!pip install -e .

def run_netsurfp_nsp3_batches(
    pkg_path: str,
    input_dir: str,
    output_dir: str,
    num_chunks: int,
    gpu: int = 1,
):
    """
    Run NetSurfP-3.0 (NSP3) on FASTA chunks in batch mode.
    Assumes NetSurfP has already been pip-installed.
    """

    import os
    from pathlib import Path
    import subprocess

    model_path = Path(pkg_path) / "models" / "nsp3.pth"
    input_dir = Path(input_dir)
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    if not model_path.exists():
        raise FileNotFoundError(f"NSP3 model not found: {model_path}")

    for i in range(1, num_chunks + 1):
        chunk_path = input_dir / f"chunk_{i}.fasta"
        if not chunk_path.exists():
            print(f"âŒ Missing {chunk_path}, skipping")
            continue

        batch_id = f"{i:02d}"
        print(f"ðŸš€ NSP3 batch {batch_id}: {chunk_path.name}")

        cmd = [
            "python", "nsp3.py",
            "-m", str(model_path),
            "-i", str(chunk_path),
            "-o", str(output_dir),
            "-w", batch_id,
            "-gpu", str(gpu),
        ]

        subprocess.run(cmd, check=True)

        print(f"âœ… Finished batch {batch_id}\n")

    print("ðŸŽ‰ NSP3 all batches complete!")

import os
import re
import json
import math
import glob
import shlex
import random
import shutil
import hashlib
import textwrap
import subprocess
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from Bio.Seq import Seq
from Bio.SeqRecord import SeqRecord
from Bio import AlignIO, SeqIO


# -----------------------------
# Utilities
# -----------------------------
def _ensure_dir(p: str | Path) -> Path:
    p = Path(p)
    p.mkdir(parents=True, exist_ok=True)
    return p

def _run(cmd: List[str] | str, *, cwd: Optional[str] = None, stdout_path: Optional[Path] = None) -> None:
    if isinstance(cmd, str):
        cmd = shlex.split(cmd)
    if stdout_path is not None:
        stdout_path.parent.mkdir(parents=True, exist_ok=True)
        with open(stdout_path, "w") as out, open(stdout_path.with_suffix(".stderr.txt"), "w") as err:
            p = subprocess.run(cmd, cwd=cwd, stdout=out, stderr=err, text=True)
    else:
        p = subprocess.run(cmd, cwd=cwd)
    if p.returncode != 0:
        raise RuntimeError(f"Command failed ({p.returncode}): {' '.join(cmd)}")

def _seed_everything(seed: int) -> None:
    random.seed(seed)
    np.random.seed(seed)
    try:
        import torch
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)
    except Exception:
        pass

def _write_fasta(df: pd.DataFrame, id_col: str, seq_col: str, out_fa: Path) -> None:
    with open(out_fa, "w") as f:
        for _, r in df.iterrows():
            f.write(f">{str(r[id_col])}\n{str(r[seq_col])}\n")

def _ungap(x: str) -> str:
    return "".join(ch for ch in str(x).strip() if ch not in "- \t\r\n")

def _pid_gapless(q: str, s: str) -> float:
    qg, sg = _ungap(q), _ungap(s)
    if not qg or not sg:
        return 0.0
    L = min(len(qg), len(sg))
    matches = sum(1 for a, b in zip(qg[:L], sg[:L]) if a == b)
    return 100.0 * matches / L

def _comb(n: int, k: int) -> int:
    return math.comb(n, k)

def _plot_save(fig: plt.Figure, out_png: Path, dpi: int = 200) -> None:
    out_png.parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(out_png, dpi=dpi, bbox_inches="tight")
    plt.close(fig)

def _normalize_desc(x: str) -> str:
    return str(x).strip().replace("|", "_").replace(" ", "_").lower()


# -----------------------------
# Mask generation (unique sampling)
# -----------------------------
def _generate_unique_masks(
    fixed_positions: List[int],
    seq_len: int,
    num_masks: int,
    k: int,
) -> List[List[int]]:
    remaining_positions = [i for i in range(seq_len) if i not in set(fixed_positions)]
    max_unique = _comb(len(remaining_positions), k)
    if num_masks > max_unique:
        raise ValueError(f"Cannot generate {num_masks} unique masks; max is {max_unique} for k={k}.")

    masks: set[Tuple[int, ...]] = set()
    attempts = 0
    max_attempts = num_masks * 30

    while len(masks) < num_masks:
        extra = tuple(sorted(random.sample(remaining_positions, k)))
        full = tuple(sorted(list(fixed_positions) + list(extra)))
        masks.add(full)
        attempts += 1
        if attempts > max_attempts:
            raise RuntimeError("Could not generate enough unique masks; decrease num_masks or k.")
    return [list(t) for t in masks]


# -----------------------------
# SOV_refine wrapper (Perl)
# -----------------------------
def _calc_sov_refine(perl_script: Path, wt_q8: str, pred_q8: str, work_dir: Path) -> float:
    if not isinstance(pred_q8, str) or len(pred_q8) == 0:
        return float("nan")

    wt_fa = work_dir / "wt_q8.fasta"
    pr_fa = work_dir / "pred_q8.fasta"

    wt_fa.write_text(f">WT\n{wt_q8}\n")
    pr_fa.write_text(f">pred\n{pred_q8}\n")

    p = subprocess.run(
        ["perl", str(perl_script), str(wt_fa), str(pr_fa), "1"],
        capture_output=True,
        text=True,
        cwd=str(work_dir),
    )
    if p.returncode != 0:
        return float("nan")
    for line in (p.stdout or "").splitlines():
        if line.startswith("SOV_refine\t"):
            try:
                return float(line.split("\t")[1])
            except Exception:
                return float("nan")
    return float("nan")


# -----------------------------
# Q8 heatmap (frequency)
# -----------------------------
def _plot_q8_frequency_heatmap(q8_list: List[str], wt_q8: str, title: str, out_png: Path) -> None:
    L = len(wt_q8)
    q8_list = [str(s)[:L].ljust(L, "-") for s in q8_list if isinstance(s, str)]
    if len(q8_list) == 0:
        return

    q8_states = list("HGIETBSTC")
    st2i = {s: i for i, s in enumerate(q8_states)}

    encoded = np.array([[st2i.get(ch, -1) for ch in s] for s in q8_list], dtype=int)
    freq = np.zeros((len(q8_states), L), dtype=float)
    for i in range(len(q8_states)):
        freq[i] = (encoded == i).sum(axis=0)
    freq /= max(1, len(q8_list))

    fig = plt.figure(figsize=(16, 5))
    ax = fig.add_axes([0.06, 0.20, 0.88, 0.72])

    im = ax.imshow(
        freq,
        aspect="auto",
        origin="upper",
        extent=[0.5, L + 0.5, len(q8_states) + 0.5, 0.5],
    )
    ax.set_xticks(np.arange(1, L + 1))
    ax.set_xticklabels(list(wt_q8), rotation=0)
    ax.set_yticks(np.arange(1, len(q8_states) + 1))
    ax.set_yticklabels(q8_states)
    ax.set_xlabel("Residue Position (Aligned to WT)")
    ax.set_ylabel("Q8 State")
    ax.set_title(title)

    cbar = fig.colorbar(im, ax=ax, fraction=0.025, pad=0.02)
    cbar.set_label("Q8 state frequency")

    _plot_save(fig, out_png, dpi=250)


# -----------------------------
# TANGO parsing helpers (same logic as your notebook)
# -----------------------------
def _has_aggregating_segment(tango_txt: Path, threshold: float = 5.0, window_sizes: Tuple[int, ...] = (5, 6)) -> bool:
    vals: List[float] = []
    with open(tango_txt) as f:
        lines = f.read().splitlines()[1:]  # skip header
    for ln in lines:
        parts = ln.strip().split()
        try:
            vals.append(float(parts[5]))
        except Exception:
            continue
    if not vals:
        return False

    s = pd.Series(vals)
    for w in window_sizes:
        for i in range(len(s) - w + 1):
            if (s.iloc[i:i + w] >= threshold).all():
                return True
    return False

def _run_tango_one(
    tango_binary: Path,
    seq_id: str,
    sequence: str,
    out_dir: Path,
    agg_threshold: float = 5.0,
) -> Tuple[Optional[Path], bool]:
    out_dir.mkdir(parents=True, exist_ok=True)
    safe = str(seq_id).replace("|", "_").replace(" ", "_")
    out_txt = out_dir / f"{safe}.txt"
    out_raw = out_dir / f"{safe}.out"

    cmd = (
        f'"{tango_binary}" {safe} '
        f'ct="N" nt="N" ph="7.0" te="310.15" io="0.165" seq="{sequence}" > "{out_raw}"'
    )
    os.system(cmd)

    produced_txt = Path(f"{safe}.txt")
    if not produced_txt.exists():
        return None, False

    produced_txt.replace(out_txt)
    passed = not _has_aggregating_segment(out_txt, threshold=agg_threshold)
    return out_txt, passed


# -----------------------------
# Protein-Sol wrapper (run wrapper script + parse seq_prediction.txt)
# -----------------------------
def _run_protein_sol(
    protein_sol_dir: Path,
    fasta_in: Path,
    work_dir: Path,
) -> pd.DataFrame:
    work_dir = _ensure_dir(work_dir)
    # Copy tools into work_dir (as in your notebook)
    for fp in protein_sol_dir.glob("*"):
        if fp.is_file():
            shutil.copy(fp, work_dir / fp.name)

    wrapper = work_dir / "multiple_prediction_wrapper_export.sh"
    if not wrapper.exists():
        raise FileNotFoundError(f"Protein-Sol wrapper not found in {work_dir}")

    _run(["chmod", "+x", str(wrapper)], cwd=str(work_dir))
    _run([str(wrapper), str(fasta_in)], cwd=str(work_dir))

    pred_txt = work_dir / "seq_prediction.txt"
    if not pred_txt.exists():
        raise RuntimeError("Protein-Sol did not produce seq_prediction.txt")

    sol_results = []
    with open(pred_txt) as f:
        for line in f:
            if line.upper().startswith("SEQUENCE PREDICTIONS"):
                parts = [p.strip() for p in line.split(",")]
                desc = parts[1].lstrip(">")
                solubility = float(parts[3])
                pI = float(parts[5])
                sol_results.append((desc, solubility, pI))

    sol_df = pd.DataFrame(sol_results, columns=["desc", "Scaled_Solubility", "pI"])
    sol_df["desc_norm"] = sol_df["desc"].map(_normalize_desc)
    return sol_df

def msa_classifier_mask_pipeline(
    WT_seq: str,
    WT_Q8: str,
    out_dir: str = "/content/msa_classifier_mask_results",

    # Paths (Drive)
    ampdb_fasta_path: str = "/content/drive/My Drive/AMPDB_Master_Dataset.fasta",
    ampsorter_script: str = "/content/drive/My Drive/AMPSorter_predictor_corrected_fast.py",
    model_pt_path: str = "/content/drive/My Drive/model.pt",
    sov_refine_pl: str = "/content/drive/My Drive/SOV_refine.pl",
    netsurfp_zip: str = "/content/drive/My Drive/netsurfp-3.0.Linux.zip",
    tango_binary_drive: str = "/content/drive/My Drive/tango_x86_64_release",
    protein_sol_dir: str = "/content/drive/My Drive/Protein-Sol",

    # BLAST filters
    e_value: float = 1e-2,
    seq_identity: float = 30.0,
    query_cov: float = 80.0,

    # Thresholds
    cons_thresh: float = 0.80,
    activity_thresh: float = 0.95,
    sov_thresh: float = 0.95,
    sol_thresh: float = 0.45,

    # Mask / sampling
    mutation_alphabet: list = None,
    variants_per_mask: int = 50,
    n_variants: int = 1_000_000,
    seed: int = 42,

    # Runtime switches
    run_netsurfp: bool = True,
    run_tango: bool = True,
    run_proteinsol: bool = True,
):
    """
    Full MSAâ€“Classifierâ€“Mask pipeline with progress prints.

    Returns:
        dict with keys:
            - best_mask
            - final_df
            - dfs (intermediate DataFrames)
            - paths (CSV / PNG / FASTA outputs)
    """

    # ==========================================================
    # STEP 0 â€” Initialization
    # ==========================================================
    print("[STEP 0] Initializing pipeline and validating inputs ...")

    import os, random, math, json, shutil, subprocess
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    from pathlib import Path
    from Bio import SeqIO, AlignIO
    from Bio.Seq import Seq
    from Bio.SeqRecord import SeqRecord

    random.seed(seed)
    np.random.seed(seed)

    WT_seq = WT_seq.strip().upper()
    WT_Q8 = WT_Q8.strip().upper()
    if len(WT_seq) != len(WT_Q8):
        raise ValueError("WT_seq and WT_Q8 must have the same length")

    if mutation_alphabet is None:
        mutation_alphabet = ["K","R","H","L","I","V","F","W","A","G","P"]

    L = len(WT_seq)
    out_dir = Path(out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    dfs = {}
    paths = {}
    # ==========================================================
# STEP 1 â€” BLASTP
# ==========================================================
        # ==========================================================
    # STEP 1 â€” BLASTP (RAW-ALIGNED)
    # ==========================================================
    print("[STEP 1] Running BLASTP against AMPDBv1 ...")

    query_fa = out_dir / "query.fasta"
    query_fa.write_text(f">query_seq\n{WT_seq}\n")

    blast_db = "/content/ampdb_v1_db"
    blast_out = out_dir / "blast_hits.tsv"

    blast_cmd = [
        "blastp",
        "-query", str(query_fa),
        "-db", blast_db,
        "-task", "blastp-short",
        "-matrix", "BLOSUM62",
        "-gapopen", "11",
        "-gapextend", "1",
        "-word_size", "2",
        "-seg", "no",
        "-comp_based_stats", "0",
        "-evalue", "500",
        "-max_target_seqs", "2000",
        "-outfmt",
        "6 qseqid sacc pident length evalue bitscore score qcovs qstart qend sstart send qseq sseq stitle",
        "-out", str(blast_out)
    ]

    subprocess.run(blast_cmd, check=True)

    # -----------------------
    # helpers (IDENTICAL)
    # -----------------------
    def ungap(x: str) -> str:
        return "".join(ch for ch in str(x).strip() if ch not in "- \t\r\n")

    def pid_gapless(q: str, s: str) -> float:
        qg, sg = ungap(q), ungap(s)
        if not qg or not sg:
            return 0.0
        L = min(len(qg), len(sg))
        matches = sum(a == b for a, b in zip(qg[:L], sg[:L]))
        return 100.0 * matches / L

    # -----------------------
    # LOAD + PROCESS (RAW STYLE)
    # -----------------------
    cols = [
        "qseqid","sacc","pident","length","evalue","bitscore","score",
        "qcovs","qstart","qend","sstart","send","qseq","sseq","stitle"
    ]

    df = pd.read_csv(blast_out, sep="\t", names=cols, dtype=str)

    for c in ["pident","length","evalue","bitscore","qcovs"]:
        df[c] = pd.to_numeric(df[c], errors="coerce")

    df["pid_calc"] = [
        pid_gapless(q, s)
        for q, s in zip(df["qseq"], df["sseq"])
    ]

    # SORT FIRST (CRITICAL)
    df = df.sort_values(
        ["evalue", "bitscore", "pid_calc", "length", "qcovs"],
        ascending=[True, False, False, False, False]
    )

    # FILTER (IDENTICAL THRESHOLDS)
    L = len(WT_seq)

    df = df[
        (df["pid_calc"] >= 30) &
        (df["evalue"] <= 1e-2) &
        (df["length"] >= 0.8 * L)
    ].copy()

    dfs["blast_filtered"] = df
    df.to_csv(out_dir / "blast_filtered.csv", index=False)

    print(f"[STEP 1] BLASTP finished âœ“  (homologs = {len(df)})\n")

    # ==========================================================
    # STEP 2 â€” MSA + conservation
    # ==========================================================
    print("[STEP 2] Performing MSA and computing conservation profile ...")

    records = [SeqRecord(Seq(WT_seq), id="WT")]
    df_blast = dfs["blast_filtered"]
    for i, row in df_blast.iterrows():
        records.append(SeqRecord(Seq(row["sseq"].replace("-", "")), id=f"hit_{i}"))

    msa_input = out_dir / "msa_input.fasta"
    SeqIO.write(records, msa_input, "fasta")

    msa_output = out_dir / "msa_aligned.fasta"
    subprocess.run(
        ["clustalo", "-i", msa_input, "-o", msa_output, "--force"],
        check=True
    )

    alignment = AlignIO.read(msa_output, "fasta")
    wt_aligned = str(alignment[0].seq)

    conservation = []
    pos_map = []

    for i, aa in enumerate(wt_aligned):
        if aa == "-":
            continue
        matches = sum(rec.seq[i] == aa for rec in alignment[1:] if rec.seq[i] != "-")
        total = sum(rec.seq[i] != "-" for rec in alignment[1:])
        conservation.append(matches / total if total else 0.0)
        pos_map.append(len(pos_map))

    cons_df = pd.DataFrame({
        "Position": pos_map,
        "WT_AA": [WT_seq[p] for p in pos_map],
        "Conservation": conservation
    })

    dfs["conservation"] = cons_df
    cons_df.to_csv(out_dir / "conservation.csv", index=False)
    paths["conservation_csv"] = str(out_dir / "conservation.csv")

    print("[STEP 2] MSA & conservation finished âœ“\n")

    # ==========================================================
    # STEP 3 â€” Fixed / unfixed positions
    # ==========================================================
    print("[STEP 3] Determining fixed and unfixed positions ...")

    fixed = set(cons_df.loc[cons_df["Conservation"] >= cons_thresh, "Position"])
    fixed |= {i for i, aa in enumerate(WT_seq) if aa == "C"}

    unfixed = [i for i in range(L) if i not in fixed]

    print(f"[STEP 3] Fixed positions identified âœ“  (fixed = {len(fixed)}, unfixed = {len(unfixed)})\n")
    # ==========================================================
    # STEP 4 â€” Mask enumeration
    # ==========================================================
    print("[STEP 4] Enumerating admissible half-length masks ...")

    target_mask = math.ceil(L / 2)
    add_mask = target_mask - len(fixed)

    if add_mask < 0:
        print(
            f"[STEP 4] Fixed positions ({len(fixed)}) exceed target mask size ({target_mask}).\n"
        "         Skipping mask enumeration â€” using fixed positions as the final mask.\n"
    )
        masks = [tuple(sorted(fixed))]
    else:
        from itertools import combinations
        masks = list(combinations(unfixed, add_mask))

    print(f"[STEP 4] Mask enumeration finished âœ“  (total masks = {len(masks)})\n")


    # ==========================================================
    # STEP 5 â€” Mask scoring (batch AMPSorter call)
    # ==========================================================
    print("[STEP 5] Scoring masks with AMPSorter (batch mode) ...")

    # --------------------------------------------------
    # 5.1 Generate variants for ALL masks
    # --------------------------------------------------
    rows = []
    for midx, mask in enumerate(masks):
        mask_set = set(mask)
        for v in range(variants_per_mask):
            seq = list(WT_seq)
            for i in range(L):
                if i not in mask_set:
                    seq[i] = random.choice(mutation_alphabet)
            rows.append({
                "ID": f"mask{midx}_var{v}",
                "Sequence": "".join(seq),
                "mask_id": midx
            })

    df_mask_variants = pd.DataFrame(rows)
    mask_variants_csv = out_dir / "mask_variants.csv"
    df_mask_variants.to_csv(mask_variants_csv, index=False)


    # --------------------------------------------------
    # 5.2 Run AMPSorter ONCE (batch inference)
    # --------------------------------------------------
    mask_scored_csv = out_dir / "mask_variants_scored.csv"

    subprocess.run(
        [
            "python", ampsorter_script,
            "--raw_data_path", str(out_dir / "mask_variants.csv"),  # âœ… CORRECT
            "--model_path", "/content/drive/MyDrive/ProteoGPT/",
            "--classifier_path", model_pt_path,
            "--output_path", str(mask_scored_csv),
            "--candidate_amp_path", str(out_dir / "mask_variants_candidate_amps.csv"),
            "--batch_size", "256",
            "--num_workers", "4",
            "--max_length", "50"
        ],
        check=True
    )


    # --------------------------------------------------
    # 5.3 Aggregate mean score per mask
    # --------------------------------------------------
    df_scored = pd.read_csv(mask_scored_csv)
    df_merged = pd.merge(df_mask_variants, df_scored, on="ID")

    mask_means = (
        df_merged
        .groupby("mask_id")["Predicted Probabilities"]
        .mean()
        .reset_index()
        .sort_values("Predicted Probabilities", ascending=False)
    )

    best_mask_id = int(mask_means.iloc[0]["mask_id"])
    best_mask = list(masks[best_mask_id])

    mask_means.to_csv(out_dir / "mask_mean_scores.csv", index=False)

    print(
        f"[STEP 5] Mask scoring finished âœ“  "
        f"(best mask = {best_mask_id}, "
        f"mean prob = {mask_means.iloc[0]['Predicted Probabilities']:.3f})\n"
    )

    # ==========================================================
    # STEP 6 â€” Large-scale variant generation
    # ==========================================================
    print(f"[STEP 6] Generating {n_variants:,} variants using best mask ...")

    variants = []
    for i in range(n_variants):
        seq = list(WT_seq)
        for pos in range(L):
            if pos not in best_mask:
                seq[pos] = random.choice(mutation_alphabet)
        variants.append({"ID": f"v{i}", "Sequence": "".join(seq)})

    df_variants = pd.DataFrame(variants)
    df_variants.to_csv(out_dir / "variants.csv", index=False)
    paths["variants_csv"] = str(out_dir / "variants.csv")

    print("[STEP 6] Variant generation finished âœ“\n")

    # ==========================================================
    # STEP 7 â€” Activity filtering
    # ==========================================================
        # Ensure each variant has a stable variant_id
    if "variant_id" not in df_variants.columns:
        df_variants = df_variants.copy()
        df_variants["variant_id"] = [
            f"var_{i}" for i in range(len(df_variants))
        ]

    dfs["variants"] = df_variants

    print("[STEP 7] Filtering variants by AMPSorter activity ...")

    subprocess.run(
        [
            "python", ampsorter_script,
            "--raw_data_path", str(out_dir / "variants.csv"),
            "--model_path", "/content/drive/My Drive/ProteoGPT/",
            "--classifier_path", "/content/drive/My Drive/model.pt",
            "--output_path", str(out_dir / "variants_scored.csv"),
            "--candidate_amp_path", str(out_dir / "variants_candidate_amps.csv"),
            "--batch_size", "256",
            "--num_workers", "4",
            "--max_length", "50"
        ],
    check=True
    )


    df_scored = pd.read_csv(out_dir / "variants_scored.csv")
    df_active = df_scored[df_scored["Predicted Probabilities"] >= activity_thresh]
    # Ensure active variants have stable variant_id (required for NetSurfP / Q8 merge)
    if "variant_id" not in df_active.columns:
        df_active = df_active.copy()
        df_active["variant_id"] = [
            f"var_{i}" for i in range(len(df_active))
        ]
    dfs["active_variants"] = df_active
    df_active.to_csv(out_dir / "variants_active.csv", index=False)
    paths["variants_active_csv"] = str(out_dir / "variants_active.csv")

    print(f"[STEP 7] Activity filtering finished âœ“  (active = {len(df_active)})\n")
        # ==========================================================
    # STEP 7.5 â€” Write FASTA chunks for NetSurfP
    # ==========================================================
    print("[STEP 7.5] Writing FASTA chunks for NetSurfP ...")

    from pathlib import Path
    from math import ceil

    df_active = dfs["active_variants"]

    assert len(df_active) > 0, "No active variants to send to NetSurfP"

    # Directory expected by STEP 8
    fasta_chunks_dir = out_dir / "fasta_chunks"
    fasta_chunks_dir.mkdir(parents=True, exist_ok=True)

    # Parameters (match raw setup)
    variants_per_chunk = 50   # safe default for GPU
    seq_col = "Sequence"      # âš ï¸ change if your column name differs

    # Write chunked FASTA files
    num_chunks = ceil(len(df_active) / variants_per_chunk)

    for i in range(num_chunks):
        chunk_df = df_active.iloc[
            i * variants_per_chunk : (i + 1) * variants_per_chunk
        ]

        chunk_path = fasta_chunks_dir / f"chunk_{i+1}.fasta"

        with open(chunk_path, "w") as f:
            for j, row in chunk_df.iterrows():
                header = f">var_{j}"
                seq = row[seq_col]
                f.write(f"{header}\n{seq}\n")

    print(f"[STEP 7.5] FASTA chunks written âœ“  (chunks = {num_chunks})")

    # ==========================================================
    # STEP 8â€“10 â€” Structural & physicochemical filters
    # ==========================================================
    # ==========================================================
    # STEP 8 â€” NetSurfP-3.0 (NSP3, Q8 prediction)
    # ==========================================================

    print("[STEP 8] Running NetSurfP-3.0 (NSP3 batch mode) ...")

    import glob, os, subprocess
    from pathlib import Path

    # Paths (IDENTICAL to raw code)
    pkg_path = "/content/NetSurfP3/NetSurfP-3.0_standalone"
    model_path = f"{pkg_path}/models/nsp3.pth"

    input_dir = out_dir / "fasta_chunks"      # created in STEP 7.5
    output_dir = out_dir / "netsurfp_out"
    output_dir.mkdir(parents=True, exist_ok=True)

    # Collect FASTA chunks
    fasta_files = sorted(input_dir.glob("chunk_*.fasta"))
    num_chunks = len(fasta_files)

    print(f"[STEP 8] Number of FASTA chunks: {num_chunks}")

    assert num_chunks > 0, "No FASTA chunks found for NetSurfP"

    # Loop EXACTLY like raw code
    for i, fasta_path in enumerate(fasta_files, start=1):

        batch_id = f"{i:02d}"
        print(f"ðŸš€ NSP3 batch {batch_id} â†’ {fasta_path.name}")

        cmd = [
            "python", f"{pkg_path}/nsp3.py",
            "-m", model_path,
            "-i", str(fasta_path),
            "-o", str(output_dir),
            "-w", batch_id
        ]

        res = subprocess.run(
             cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
          text=True
           )

        if res.returncode != 0:
           print("âŒ NetSurfP failed")
           print("STDOUT:\n", res.stdout)
           print("STDERR:\n", res.stderr)
           raise RuntimeError("NetSurfP execution failed")

        print(f"âœ… Finished batch {batch_id}\n")

    print("ðŸŽ‰ NSP3 all batches complete!")
    print("[STEP 8] NSP3 execution finished âœ“")



    # ==========================================================
    # STEP 8.5 â€” Extract Q8 predictions from NetSurfP JSONs
    # (ROBUST to all NetSurfP JSON formats)
    # ==========================================================
    print("[STEP 8.5] Extracting Q8 predictions from NetSurfP JSONs ...")

    import json
    import pandas as pd
    from pathlib import Path

    netsurfp_out = out_dir / "netsurfp_out"
    json_files = sorted(netsurfp_out.rglob("*.json"))

    assert len(json_files) > 0, "No NetSurfP JSON outputs found"

    q8_records = []

    for jf in json_files:
        with open(jf) as f:
            data = json.load(f)

        # ---- Normalize NetSurfP output to a list of records ----
        if isinstance(data, list):
            records = data

        elif isinstance(data, dict):
            if "results" in data and isinstance(data["results"], list):
                records = data["results"]
            else:
                records = [data]

        else:
            raise RuntimeError(
                f"Unrecognized NetSurfP JSON format in {jf}"
            )

        # ---- Extract Q8 only ----
        for rec in records:
            if not isinstance(rec, dict):
                continue

            if "id" not in rec or "q8" not in rec:
                continue

            q8_records.append({
                "variant_id": rec["id"],   # e.g. 0002_var_22
                "Q8_pred": rec["q8"]       # Q8 string
            })

    df_q8 = pd.DataFrame(q8_records)

    assert len(df_q8) > 0, "No Q8 predictions extracted from NetSurfP outputs"

    # ---- Normalize IDs to match FASTA headers (e.g. var_22) ----
    df_q8["variant_id"] = df_q8["variant_id"].apply(
        lambda x: x.split("_", 1)[-1]
    )

    df_active = dfs["active_variants"]

    assert "variant_id" in df_active.columns, \
        "df_active must contain 'variant_id' column for merging Q8"

    df_active = df_active.merge(
        df_q8,
        on="variant_id",
        how="inner"
    )

    # ---- Final sanity check for SOV ----
    assert df_active["Q8_pred"].apply(len).nunique() == 1, \
        "Inconsistent Q8 lengths â€” cannot compute SOV"

    dfs["active_variants"] = df_active

    print(f"[STEP 8.5] Q8 merged into active variants âœ“  (n = {len(df_active)})")


    # ==========================================================
    # STEP 9 â€” SOV_refine filtering
    # ==========================================================
    if run_netsurfp:
        print("[STEP 9] Computing SOV_refine scores vs WT ...")

        sov_scores = []
        sov_dir = _ensure_dir(out_dir / "sov_tmp")

        for _, r in df_active.iterrows():
            sov = _calc_sov_refine(
                perl_script=Path(sov_refine_pl),
                wt_q8=WT_Q8,
                pred_q8=r["Q8_pred"],
                work_dir=sov_dir,
            )
            sov_scores.append(sov)

        df_active["SOV_refine"] = sov_scores
        df_sov = df_active[df_active["SOV_refine"] >= sov_thresh].copy()

        dfs["sov_pass"] = df_sov
        df_sov.to_csv(out_dir / "variants_sov_pass.csv", index=False)

        print(
            f"[STEP 9] SOV filtering finished âœ“  "
            f"(passed = {len(df_sov)})\n"
        )

    else:
        print("[STEP 9] Skipped (run_netsurfp=False)\n")
        df_sov = df_active

    # ==========================================================
    # STEP 10 â€” Physicochemical filters (TANGO + Protein-Sol)
    # ==========================================================
    print("[STEP 10] Applying physicochemical filters (TANGO, Protein-Sol) ...")

    df_phys = df_sov.copy()

    # ---- TANGO aggregation filter ----
    if run_tango:
        print("[STEP 10a] Running TANGO aggregation filter ...")

        tango_dir = _ensure_dir(out_dir / "tango")
        tango_pass = []

        for _, r in df_phys.iterrows():
            _, passed = _run_tango_one(
                tango_binary=Path(tango_binary_drive),
                seq_id=r["ID"],
                sequence=r["Sequence"],
                out_dir=tango_dir,
            )
            tango_pass.append(passed)

        df_phys["TANGO_pass"] = tango_pass
        df_phys = df_phys[df_phys["TANGO_pass"]].copy()

        print(f"[STEP 10a] TANGO finished âœ“  (passed = {len(df_phys)})")

    else:
        print("[STEP 10a] Skipped TANGO (run_tango=False)")

    # ---- Protein-Sol filter ----
    if run_proteinsol:
        print("[STEP 10b] Running Protein-Sol solubility prediction ...")

        phys_fa = out_dir / "variants_phys.fasta"
        _write_fasta(df_phys, "ID", "Sequence", phys_fa)

        sol_df = _run_protein_sol(
            protein_sol_dir=Path(protein_sol_dir),
            fasta_in=phys_fa,
            work_dir=out_dir / "protein_sol",
        )

        df_phys["desc_norm"] = df_phys["ID"].map(_normalize_desc)
        df_phys = df_phys.merge(sol_df, on="desc_norm", how="left")
        df_phys = df_phys[df_phys["Scaled_Solubility"] >= sol_thresh].copy()

        print(
            f"[STEP 10b] Protein-Sol finished âœ“  "
            f"(passed = {len(df_phys)})"
        )

    else:
        print("[STEP 10b] Skipped Protein-Sol (run_proteinsol=False)")

    dfs["final"] = df_phys
    df_phys.to_csv(out_dir / "variants_final.csv", index=False)

    print(
        f"[STEP 10] Physicochemical filtering finished âœ“  "
        f"(final variants = {len(df_phys)})\n"
    )

#### run sanity check

res_test = msa_classifier_mask_pipeline(
    WT_seq="VCSCRLVFCRRTELRVGNCLIGGVSFTYCCTRV",
    WT_Q8="CCECCSCCCCTTCEEEEEEEETTEEEEEEECCC",

    out_dir="/content/hnp4_sanity_test",

    # sanity-test scale
    variants_per_mask=3,
    n_variants=100,
    seed=123,

    # thresholds
    activity_thresh=0.5,   #  lowered as requested

    # enable NetSurfP only
    run_netsurfp=True,
    run_tango=False,
    run_proteinsol=False,
)
